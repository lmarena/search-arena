{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Analysis\n",
    "\n",
    "**Sections 3.1, 3.2, Appendix D, E:** leaderboard analysis, controlled Bradley-Terry analysis, citation control analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import sem, t\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battle_data = pd.read_json('../data/search_arena_24k.jsonl', orient='records', lines=True)\n",
    "battle_data['timestamp'] = pd.to_datetime(battle_data['timestamp']).dt.tz_localize('UTC').dt.tz_convert('America/Los_Angeles')\n",
    "battle_data = battle_data[battle_data['winner'].notna()].reset_index(drop=True)\n",
    "\n",
    "model_mapping = {\n",
    "    \"gpt-4o-mini-search-preview\": \"gpt-4o-mini-search\",\n",
    "    \"gpt-4o-mini-search-preview-high\": \"gpt-4o-mini-search-high\",\n",
    "    \"gpt-4o-search-preview\": \"gpt-4o-search\",\n",
    "    \"gpt-4o-search-preview-high\": \"gpt-4o-search-high\",\n",
    "    \"gpt-4o-search-preview-high-loc\": \"gpt-4o-search-high-loc\",\n",
    "    \"gemini-2.0-flash-grounding\": \"gemini-2.0-flash-grounding\",\n",
    "    \"gemini-2.5-flash-preview-04-17-grounding\": \"gemini-2.5-flash-grounding\",\n",
    "    \"gemini-2.0-pro-exp-02-05-grounding\": \"gemini-2.5-pro-grounding\",\n",
    "    \"gemini-2.5-pro-exp-03-25-grounding\": \"gemini-2.5-pro-grounding\",\n",
    "    \"gemini-2.5-pro-exp-03-25-wo-search\": \"gemini-2.5-pro\"\n",
    "}\n",
    "battle_data[\"model_a\"].replace(model_mapping, inplace=True)\n",
    "battle_data[\"model_b\"].replace(model_mapping, inplace=True)\n",
    "battles_no_ties = battle_data[~battle_data['winner'].isin(['tie', 'tie (bothbad)'])]\n",
    "\n",
    "models_to_remove = ['gemini-2.5-pro'] # non-search model\n",
    "battle_data = battle_data[~battle_data['model_a'].isin(models_to_remove) & ~battle_data['model_b'].isin(models_to_remove)].reset_index(drop=True)\n",
    "\n",
    "print(f'Number of battles with votes: {len(battle_data)}')\n",
    "display(battle_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Arena Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_battle_count_by_model(battles):\n",
    "    models = pd.concat([battles['model_a'], battles['model_b']]).value_counts()\n",
    "    fig = px.bar(\n",
    "        models,\n",
    "        text_auto=\"auto\",\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        textposition='outside',\n",
    "        textfont=dict(size=15, color='black')\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(\n",
    "            title='',\n",
    "            title_font=dict(size=18, color='black'),\n",
    "            tickangle=-45,\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            linecolor='black',\n",
    "            linewidth=1.0\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Number of Battles',\n",
    "            title_font=dict(size=18, color='black'),\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray',\n",
    "            linecolor='black',\n",
    "            linewidth=1.0\n",
    "        ),\n",
    "        margin=dict(l=10, r=10, t=30, b=10),\n",
    "        height=600,\n",
    "        width=800\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "fig = visualize_battle_count_by_model(battle_data)\n",
    "fig.show()\n",
    "fig.write_image('plots/battle_count.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_average_win_rate(battles):\n",
    "    row_beats_col_freq = compute_pairwise_win_fraction(\n",
    "        battles, None, limit_show_number=None\n",
    "    )\n",
    "    fig = px.bar(\n",
    "        row_beats_col_freq.mean(axis=1).sort_values(ascending=False),\n",
    "        text_auto=\".2f\",\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        textposition='outside',\n",
    "        textfont=dict(size=15, color='black')\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(\n",
    "            title='',\n",
    "            tickangle=-45,\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            linecolor='black',\n",
    "            linewidth=1.0\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(text='Average Win Rate', font=dict(size=18, color='black')),\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray',\n",
    "            linecolor='black',\n",
    "            linewidth=1.0\n",
    "        ),        \n",
    "        margin=dict(l=10, r=10, t=10, b=10),\n",
    "        height=600,\n",
    "        width=800\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "fig = visualize_average_win_rate(battles_no_ties)\n",
    "fig.show()\n",
    "fig.write_image('plots/avg_win_rate.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_battle_count(battles, model_order):\n",
    "    ptbl = pd.pivot_table(\n",
    "        battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0\n",
    "    )\n",
    "    battle_counts = ptbl + ptbl.T\n",
    "    fig = px.imshow(\n",
    "        battle_counts.loc[model_order, model_order],\n",
    "        text_auto=True,\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        textfont=dict(size=14)    \n",
    "    )\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(\n",
    "            title=dict(text='Model B', font=dict(size=18, color='black')),\n",
    "            tickfont=dict(size=14, color='black')\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(text='Model A', font=dict(size=18, color='black')),\n",
    "            tickfont=dict(size=14, color='black')\n",
    "        ),\n",
    "        xaxis_side=\"top\",\n",
    "        margin=dict(l=20, r=20, t=20, b=20),\n",
    "        coloraxis_colorbar=dict(\n",
    "            len=1.0,\n",
    "            thickness=30,\n",
    "            title=\"\",\n",
    "            xpad=5,\n",
    "            tickfont=dict(size=14, color='black')\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "fig = visualize_battle_count(battle_data, get_model_order(battle_data))\n",
    "fig.show()\n",
    "fig.write_image('plots/battle_count_pairwise.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pairwise_win_fraction(battles, model_order):\n",
    "    row_beats_col = compute_pairwise_win_fraction(battles, model_order)\n",
    "    fig = px.imshow(\n",
    "        row_beats_col,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "        text_auto=\".2f\"\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        textfont=dict(size=14)    \n",
    "    )\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(\n",
    "            title=dict(text='Model B', font=dict(size=18, color='black')),\n",
    "            tickfont=dict(size=14, color='black')\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(text='Model A', font=dict(size=18, color='black')),\n",
    "            tickfont=dict(size=14, color='black')\n",
    "        ),\n",
    "        xaxis_side=\"top\",\n",
    "        margin=dict(l=20, r=20, t=20, b=20),\n",
    "        coloraxis_colorbar=dict(\n",
    "            len=1.0,\n",
    "            thickness=30,\n",
    "            title=\"\",\n",
    "            xpad=5,\n",
    "            tickfont=dict(size=14, color='black')\n",
    "        ),\n",
    "        height=800,\n",
    "        width=1000,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "fig = visualize_pairwise_win_fraction(battles_no_ties, get_model_order(battle_data))\n",
    "fig.show()\n",
    "fig.write_image('plots/pairwise_winrates.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bt_ratings(battle_data, anchor_model, anchor_rating=1000, num_bootstrap_samples=1000, style_elements=None):\n",
    "    if style_elements is None:\n",
    "        bt_ratings = compute_bt(battle_data)\n",
    "        offset_score = (anchor_rating - bt_ratings[anchor_model])\n",
    "        bt_ratings += offset_score\n",
    "        bt_ratings_bootstrap = compute_bootstrap_bt(battle_data, num_round=100, offset=offset_score)\n",
    "        style_coef_bootstrap = None\n",
    "    else:\n",
    "        bt_ratings, _ = compute_style_control(battle_data, style_elements=style_elements)\n",
    "        offset_score = (anchor_rating - bt_ratings[anchor_model])\n",
    "        bt_ratings += offset_score\n",
    "        bt_ratings_bootstrap, style_coef_bootstrap = compute_bootstrap_style_control(battle_data, style_elements=style_elements, num_round=num_bootstrap_samples, offset=offset_score)\n",
    "    return bt_ratings_bootstrap, style_coef_bootstrap\n",
    "\n",
    "def get_leaderboard_table(bt_ratings_bootstrap):\n",
    "    model_order = list(bt_ratings_bootstrap.columns)\n",
    "    model_rating_q025 = bt_ratings_bootstrap.quantile(0.025).round(2)\n",
    "    model_rating_q975 = bt_ratings_bootstrap.quantile(0.975).round(2)\n",
    "    bt_ratings = bt_ratings_bootstrap.mean().round(2)\n",
    "    bt_var = bt_ratings_bootstrap.var().round(2)\n",
    "\n",
    "    ranking = {}\n",
    "    for i, model_a in enumerate(model_order):\n",
    "        ranking[model_a] = 1\n",
    "        for j, model_b in enumerate(model_order):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if model_rating_q025[model_b] > model_rating_q975[model_a]:\n",
    "                ranking[model_a] += 1\n",
    "\n",
    "    leaderboard_table = pd.DataFrame(\n",
    "        {\n",
    "            \"rating\": bt_ratings,\n",
    "            \"variance\": bt_var,\n",
    "            \"rating_q975\": model_rating_q975,\n",
    "            \"rating_q025\": model_rating_q025,\n",
    "            \"num_battles\": battle_data[\"model_a\"].value_counts().add(battle_data[\"model_b\"].value_counts(), fill_value=0),\n",
    "            \"final_ranking\": pd.Series(ranking),\n",
    "        }\n",
    "    )\n",
    "    leaderboard_table = leaderboard_table.sort_values(by='rating', ascending=False)\n",
    "    return leaderboard_table\n",
    "\n",
    "bt_ratings_bootstrap, _ = get_bt_ratings(battle_data, anchor_model='gpt-4o-search')\n",
    "leaderboard_table = get_leaderboard_table(bt_ratings_bootstrap)\n",
    "display(leaderboard_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bootstrap_elo_rating(bt_ratings_bootstrap):\n",
    "    bars = (\n",
    "        pd.DataFrame(\n",
    "            dict(\n",
    "                lower=bt_ratings_bootstrap.quantile(0.025),\n",
    "                rating=bt_ratings_bootstrap.mean(),\n",
    "                upper=bt_ratings_bootstrap.quantile(0.975),\n",
    "            )\n",
    "        )\n",
    "        .reset_index(names=\"model\")\n",
    "        .sort_values(\"rating\", ascending=False)\n",
    "    )\n",
    "    bars[\"error_y\"] = bars[\"upper\"] - bars[\"rating\"]\n",
    "    bars[\"error_y_minus\"] = bars[\"rating\"] - bars[\"lower\"]\n",
    "    bars[\"rating_rounded\"] = np.round(bars[\"rating\"])\n",
    "    fig = px.scatter(\n",
    "        bars,\n",
    "        x=\"model\",\n",
    "        y=\"rating\",\n",
    "        error_y=\"error_y\",\n",
    "        error_y_minus=\"error_y_minus\",\n",
    "        text=\"rating_rounded\",\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=10, color=\"royalblue\"),\n",
    "        line=dict(width=1, color=\"royalblue\"),\n",
    "        textposition='top left',\n",
    "        textfont=dict(size=16, color='black')\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(\n",
    "            title='',\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            linecolor='black',\n",
    "            linewidth=1.0,\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(text='Rating', font=dict(size=18, color='black')),\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            linecolor='black',\n",
    "            linewidth=1.0,\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray'\n",
    "        ),\n",
    "        margin=dict(l=10, r=10, t=10, b=10),\n",
    "        height=500,\n",
    "        width=1200,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "bt_ratings_bootstrap, _ = get_bt_ratings(battle_data, anchor_model='gpt-4o-search')\n",
    "fig = visualize_bootstrap_elo_rating(bt_ratings_bootstrap)\n",
    "fig.show()\n",
    "fig.write_image('plots/elo_ratings.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leaderboard Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Leaderboard (English)')\n",
    "battle_data_english = battle_data[battle_data['languages'].apply(lambda x: x[0]=='English' if x else False)]\n",
    "bt_ratings_bootstrap_english, _ = get_bt_ratings(battle_data_english, anchor_model='gpt-4o-search')\n",
    "leaderboard_table_english = get_leaderboard_table(bt_ratings_bootstrap_english)\n",
    "display(leaderboard_table_english)\n",
    "\n",
    "print('Leaderboard (non English)')\n",
    "battle_data_nonenglish = battle_data[battle_data['languages'].apply(lambda x: x[0]!='English' if x else False)]\n",
    "bt_ratings_bootstrap_nonenglish, _ = get_bt_ratings(battle_data_nonenglish, anchor_model='gpt-4o-search')\n",
    "leaderboard_table_nonenglish = get_leaderboard_table(bt_ratings_bootstrap_nonenglish)\n",
    "display(leaderboard_table_nonenglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Leaderboard (Factual)')\n",
    "battle_data_fact = battle_data[battle_data['primary_intent'].isin(['Factual Lookup', 'Info Synthesis'])]\n",
    "bt_ratings_bootstrap_fact, _ = get_bt_ratings(battle_data_fact, anchor_model='gpt-4o-search')\n",
    "leaderboard_table_fact = get_leaderboard_table(bt_ratings_bootstrap_fact)\n",
    "display(leaderboard_table_fact)\n",
    "\n",
    "print('Leaderboard (non-Factual)')\n",
    "battle_data_nonfact = battle_data[~battle_data['primary_intent'].isin(['Factual Lookup', 'Info Synthesis'])]\n",
    "bt_ratings_bootstrap_nonfact, _ = get_bt_ratings(battle_data_nonfact, anchor_model='gpt-4o-search')\n",
    "leaderboard_table_nonfact = get_leaderboard_table(bt_ratings_bootstrap_nonfact)\n",
    "display(leaderboard_table_nonfact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Win Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(battle_data, model_1, model_2):\n",
    "    curr_data = battle_data[battle_data['winner'].isin(['model_a', 'model_b'])]\n",
    "    curr_data = curr_data[curr_data['model_a'] != curr_data['model_b']]\n",
    "    winner_a_count = curr_data[curr_data['winner'] == 'model_a'].pivot_table(index='model_a', columns='model_b', values='winner', aggfunc='count')\n",
    "    winner_b_count = curr_data[curr_data['winner'] == 'model_b'].pivot_table(index='model_a', columns='model_b', values='winner', aggfunc='count')\n",
    "    winner_count = winner_a_count + winner_b_count.T # model_a > model_b\n",
    "\n",
    "    win_count_1, total_count_1 = winner_count.loc[model_1].sum(), winner_count.loc[model_1].sum() + winner_count.T.loc[model_1].sum()\n",
    "    win_count_2, total_count_2 = winner_count.loc[model_2].sum(), winner_count.loc[model_2].sum() + winner_count.T.loc[model_2].sum()\n",
    "\n",
    "    battle_data_1 = [1] * int(win_count_1) + [0] * int(total_count_1 - win_count_1)\n",
    "    battle_data_2 = [1] * int(win_count_2) + [0] * int(total_count_2 - win_count_2)\n",
    "\n",
    "    win_rate_1 = win_count_1 / total_count_1\n",
    "    win_rate_2 = win_count_2 / total_count_2\n",
    "\n",
    "    print(f'{model_1} winrate: {round(win_rate_1, 2)}')\n",
    "    print(f'{model_2} winrate: {round(win_rate_2, 2)}')\n",
    "\n",
    "    print(f't-test results (winrate_1 > winrate_2)')\n",
    "    print(ttest_ind(battle_data_1, battle_data_2, alternative='greater'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search context analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(battle_data, 'sonar-pro-high', 'sonar-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(battle_data, 'gpt-4o-search-high', 'gpt-4o-search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Win rate by intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battle_data_factual = battle_data[battle_data['primary_intent'].isin(['Factual Lookup', 'Info Synthesis'])]\n",
    "bootstrap_winrates(battle_data_factual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battle_data_nonfactual = battle_data[~battle_data['primary_intent'].isin(['Factual Lookup', 'Info Synthesis'])]\n",
    "bootstrap_winrates(battle_data_nonfactual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_response_length_style(battle_data, \"response_length\") # average response length of assistant messages\n",
    "add_num_citations_style(battle_data, \"citation_count\") # number of urls returned by the web search\n",
    "add_domain_style(battle_data, \"cites\") # whether the models cite a domain group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_a = pd.DataFrame({'model': battle_data['model_a'],\n",
    "                             'response_len': battle_data['conv_metadata'].apply(lambda x: x['response_length_a']),\n",
    "                             'citation_count': battle_data['conv_metadata'].apply(lambda x: x['citation_count_a']),\n",
    "                             **{f'cites_{domain}': battle_data['conv_metadata'].apply(lambda x: x[f'cites_{domain}_a']) for domain in DOMAIN_CATEGORIES}\n",
    "})\n",
    "model_data_b = pd.DataFrame({'model': battle_data['model_b'],\n",
    "                             'response_len': battle_data['conv_metadata'].apply(lambda x: x['response_length_b']),\n",
    "                             'citation_count': battle_data['conv_metadata'].apply(lambda x: x['citation_count_b']),\n",
    "                             **{f'cites_{domain}': battle_data['conv_metadata'].apply(lambda x: x[f'cites_{domain}_b']) for domain in DOMAIN_CATEGORIES}\n",
    "})\n",
    "model_data = pd.concat([model_data_a, model_data_b])\n",
    "model_data = model_data[~model_data['model'].isin(['gpt-4o-search-high-loc'])]\n",
    "model_data.replace({'gemini-2.5-pro-grounding': 'gemini-2.5-pro',\n",
    "                    'gemini-2.5-flash-grounding': 'gemini-2.5-flash',\n",
    "                    'gemini-2.0-flash-grounding': 'gemini-2.0-flash'}, inplace=True)\n",
    "\n",
    "family_mapping = {\n",
    "    'sonar': 'Perplexity',\n",
    "    'sonar-pro': 'Perplexity',\n",
    "    'sonar-pro-high': 'Perplexity',\n",
    "    'sonar-reasoning': 'Perplexity',\n",
    "    'sonar-reasoning-pro-high': 'Perplexity',\n",
    "    'gpt-4o-mini-search': 'OpenAI',\n",
    "    'gpt-4o-search': 'OpenAI',\n",
    "    'gpt-4o-search-high': 'OpenAI',\n",
    "    'gemini-2.0-flash': 'Google',\n",
    "    'gemini-2.5-flash': 'Google',\n",
    "    'gemini-2.5-pro': 'Google',\n",
    "}\n",
    "\n",
    "model_data['model_family'] = model_data['model'].map(family_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 0: Setup ===\n",
    "model_family_order = ['Perplexity', 'Google', 'OpenAI']\n",
    "color_map = {\n",
    "    'Google': '#e74c3c',\n",
    "    'Perplexity': '#268bd2',\n",
    "    'OpenAI': '#2ca02c'\n",
    "}\n",
    "\n",
    "# === STEP 1: Compute group stats ===\n",
    "group_stats = model_data.groupby(\"model\").agg(\n",
    "    mean_response_len=('response_len', 'mean'),\n",
    "    se_response_len=('response_len', sem),\n",
    "    mean_citation_count=('citation_count', 'mean'),\n",
    "    se_citation_count=('citation_count', sem),\n",
    "    count=('response_len', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# === STEP 2: Compute 95% CI\n",
    "group_stats['t_score'] = t.ppf(0.975, df=group_stats['count'] - 1)\n",
    "group_stats['ci_response_len'] = group_stats['t_score'] * group_stats['se_response_len']\n",
    "group_stats['ci_citation_count'] = group_stats['t_score'] * group_stats['se_citation_count']\n",
    "\n",
    "# === STEP 3: Add model_family and sort\n",
    "model_family_map = model_data.drop_duplicates(\"model\")[[\"model\", \"model_family\"]]\n",
    "summary_df = pd.merge(group_stats, model_family_map, on=\"model\")\n",
    "summary_df['model_family'] = pd.Categorical(summary_df['model_family'], categories=model_family_order, ordered=True)\n",
    "summary_df = summary_df.sort_values(by=[\"model_family\", \"mean_response_len\"], ascending=[True, False])\n",
    "summary_df['model'] = pd.Categorical(summary_df['model'], categories=summary_df['model'], ordered=True)\n",
    "\n",
    "# === STEP 4: Create subplots with independent y-axes\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    shared_yaxes=False,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# === STEP 5: Add bars for both metrics ===\n",
    "for i, row in summary_df.iterrows():\n",
    "    color = color_map[row['model_family']]\n",
    "    # Response Length\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[row['model']],\n",
    "            y=[row['mean_response_len']],\n",
    "            name=row['model_family'],\n",
    "            marker_color=color,\n",
    "            error_y=dict(type='data', array=[row['ci_response_len']], thickness=1.5),\n",
    "            opacity=0.9,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    # Number of Citations\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[row['model']],\n",
    "            y=[row['mean_citation_count']],\n",
    "            name=row['model_family'],\n",
    "            marker_color=color,\n",
    "            error_y=dict(type='data', array=[row['ci_citation_count']], thickness=1.5),\n",
    "            opacity=0.9,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# === STEP 6: Style updates ===\n",
    "for axis in ['xaxis', 'xaxis2']:\n",
    "    fig.update_layout({axis: dict(\n",
    "        tickangle=-45,\n",
    "        tickfont=dict(size=16, color='black'),\n",
    "        linecolor='black'\n",
    "    )})\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Response Length (num words)\",\n",
    "    title_font=dict(size=20, color='black'),\n",
    "    tickfont=dict(size=16, color='black'),\n",
    "    showgrid=True,\n",
    "    gridcolor='lightgray',\n",
    "    zeroline=False,\n",
    "    linecolor='black',\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Citation Count\",\n",
    "    title_font=dict(size=20, color='black'),\n",
    "    tickfont=dict(size=16, color='black'),\n",
    "    showgrid=True,\n",
    "    gridcolor='lightgray',\n",
    "    zeroline=False,\n",
    "    linecolor='black',\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    "    font=dict(size=14, color='black'),\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image('plots/length_citation_count.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_data = battle_data[~battle_data['model_a'].isin(['gpt-4o-search-high-loc']) & ~battle_data['model_b'].isin(['gpt-4o-search-high-loc'])]\n",
    "curr_data.replace({'gemini-2.5-pro-grounding': 'gemini-2.5-pro',\n",
    "                    'gemini-2.5-flash-grounding': 'gemini-2.5-flash',\n",
    "                    'gemini-2.0-flash-grounding': 'gemini-2.0-flash'}, inplace=True)\n",
    "bt_ratings_bootstrap, _ = get_bt_ratings(curr_data, anchor_model='gpt-4o-search')\n",
    "\n",
    "# === STEP 1: Score stats from bootstrap ===\n",
    "bt_summary = bt_ratings_bootstrap.describe(percentiles=[0.025, 0.975]).T\n",
    "bt_summary = bt_summary[['mean']]\n",
    "bt_summary['ci_lower'] = bt_ratings_bootstrap.quantile(0.025)\n",
    "bt_summary['ci_upper'] = bt_ratings_bootstrap.quantile(0.975)\n",
    "bt_summary['ci_y'] = (bt_summary['ci_upper'] - bt_summary['ci_lower']) / 2\n",
    "bt_summary = bt_summary.reset_index().rename(columns={'index': 'model', 'mean': 'score'})\n",
    "\n",
    "# === STEP 2: Response length stats ===\n",
    "grouped = model_data.groupby('model')['response_len']\n",
    "length_mean = grouped.mean()\n",
    "length_se = grouped.apply(sem)\n",
    "length_n = grouped.count()\n",
    "t_scores = t.ppf(0.975, df=length_n - 1)\n",
    "ci_x = t_scores * length_se\n",
    "\n",
    "length_summary = pd.DataFrame({\n",
    "    'model': length_mean.index,\n",
    "    'response_len': length_mean.values,\n",
    "    'ci_x': ci_x.values\n",
    "})\n",
    "\n",
    "# === STEP 3: Merge summaries ===\n",
    "merged = pd.merge(bt_summary, length_summary, on='model')\n",
    "\n",
    "# === STEP 4: Plot ===\n",
    "fig = px.scatter(\n",
    "    merged,\n",
    "    x='response_len',\n",
    "    y='score',\n",
    "    error_x='ci_x',\n",
    "    error_y='ci_y',\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=10, color='#268bd2', line=dict(width=1, color='black')),\n",
    "    textposition='top center',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title=dict(text='Average Response Length (words)', font=dict(size=18, color='black')),\n",
    "        tickfont=dict(size=16),\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgray'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text='BT Score Estimate', font=dict(size=18, color='black')),\n",
    "        tickfont=dict(size=16),\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgray'\n",
    "    ),\n",
    "    font=dict(size=14, color='black'),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    margin=dict(l=10, r=10, t=10, b=10),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# === STEP 5: Fit linear regression ===\n",
    "X = merged['response_len']\n",
    "y = merged['score']\n",
    "X_const = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_const).fit()\n",
    "\n",
    "\n",
    "# === STEP 6: Compute prediction line and CI ===\n",
    "x_range = np.linspace(X.min(), X.max(), 100)\n",
    "x_range_const = sm.add_constant(x_range)\n",
    "predictions = model.get_prediction(x_range_const)\n",
    "pred_summary = predictions.summary_frame(alpha=0.05)\n",
    "y_pred = pred_summary['mean']\n",
    "ci_lower = pred_summary['mean_ci_lower']\n",
    "ci_upper = pred_summary['mean_ci_upper']\n",
    "\n",
    "\n",
    "# === STEP 7: Plot ===\n",
    "fig.add_scatter(\n",
    "    x=x_range,\n",
    "    y=y_pred,\n",
    "    mode='lines',\n",
    "    line=dict(color='black', width=2, dash='dash'),\n",
    "    name='Linear Fit',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add upper and lower bound\n",
    "fig.add_scatter(\n",
    "    x=np.concatenate([x_range, x_range[::-1]]),\n",
    "    y=np.concatenate([ci_upper, ci_lower[::-1]]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(0, 0, 0, 0.1)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add regression equation\n",
    "slope = model.params['response_len']\n",
    "intercept = model.params['const']\n",
    "equation_text = f\"score = {slope:.3f} × length + {intercept:.2f}\"\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=equation_text,\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.05, y=0.95,\n",
    "    showarrow=False,\n",
    "    font=dict(size=18, color=\"black\"),\n",
    "    bgcolor=\"white\",\n",
    "    bordercolor=\"black\",\n",
    "    borderwidth=1,\n",
    "    borderpad=4\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(model.summary())\n",
    "fig.write_image('plots/score_vs_length.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: Score stats from bootstrap ===\n",
    "bt_summary = bt_ratings_bootstrap.describe(percentiles=[0.025, 0.975]).T\n",
    "bt_summary = bt_summary[['mean']]\n",
    "bt_summary['ci_lower'] = bt_ratings_bootstrap.quantile(0.025)\n",
    "bt_summary['ci_upper'] = bt_ratings_bootstrap.quantile(0.975)\n",
    "bt_summary['ci_y'] = (bt_summary['ci_upper'] - bt_summary['ci_lower']) / 2\n",
    "bt_summary = bt_summary.reset_index().rename(columns={'index': 'model', 'mean': 'score'})\n",
    "\n",
    "# === STEP 2: Response length stats ===\n",
    "grouped = model_data[model_data['citation_count']!=0].groupby('model')['citation_count']\n",
    "cit_count_mean = grouped.mean()\n",
    "cit_count_se = grouped.apply(sem)\n",
    "cit_count_n = grouped.count()\n",
    "t_scores = t.ppf(0.975, df=cit_count_n - 1)\n",
    "ci_x = t_scores * cit_count_se\n",
    "\n",
    "cit_count_summary = pd.DataFrame({\n",
    "    'model': cit_count_mean.index,\n",
    "    'citation_count': cit_count_mean.values,\n",
    "    'ci_x': ci_x.values\n",
    "})\n",
    "\n",
    "# === STEP 3: Merge summaries ===\n",
    "merged = pd.merge(bt_summary, cit_count_summary, on='model')\n",
    "\n",
    "# === STEP 4: Plot ===\n",
    "fig = px.scatter(\n",
    "    merged,\n",
    "    x='citation_count',\n",
    "    y='score',\n",
    "    error_x='ci_x',\n",
    "    error_y='ci_y',\n",
    ")\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=10, color='#268bd2', line=dict(width=1, color='black')),\n",
    "    textposition='top center',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title=dict(text='Average Number of Citations', font=dict(size=18, color='black')),\n",
    "        tickfont=dict(size=16),\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgray'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text='BT Score Estimate', font=dict(size=18, color='black')),\n",
    "        tickfont=dict(size=16),\n",
    "        linecolor='black',\n",
    "        gridcolor='lightgray'\n",
    "    ),\n",
    "    font=dict(size=14, color='black'),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    margin=dict(l=10, r=10, t=10, b=10),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# === STEP 5: Fit linear regression ===\n",
    "X = merged['citation_count']\n",
    "y = merged['score']\n",
    "X_const = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_const).fit()\n",
    "\n",
    "# === STEP 6: Compute prediction line and CI ===\n",
    "x_range = np.linspace(X.min(), X.max(), 100)\n",
    "x_range_const = sm.add_constant(x_range)\n",
    "predictions = model.get_prediction(x_range_const)\n",
    "pred_summary = predictions.summary_frame(alpha=0.05)\n",
    "y_pred = pred_summary['mean']\n",
    "ci_lower = pred_summary['mean_ci_lower']\n",
    "ci_upper = pred_summary['mean_ci_upper']\n",
    "\n",
    "# === STEP 7: Plot ===\n",
    "fig.add_scatter(\n",
    "    x=x_range,\n",
    "    y=y_pred,\n",
    "    mode='lines',\n",
    "    line=dict(color='black', width=2, dash='dash'),\n",
    "    name='Linear Fit',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add upper and lower bound\n",
    "fig.add_scatter(\n",
    "    x=np.concatenate([x_range, x_range[::-1]]),\n",
    "    y=np.concatenate([ci_upper, ci_lower[::-1]]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(0, 0, 0, 0.1)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    hoverinfo=\"skip\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Add regression equation\n",
    "slope = model.params['citation_count']\n",
    "intercept = model.params['const']\n",
    "equation_text = f\"score = {slope:.3f} × citations + {intercept:.2f}\"\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=equation_text,\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.05, y=0.95,\n",
    "    showarrow=False,\n",
    "    font=dict(size=18, color=\"black\"),\n",
    "    bgcolor=\"white\",\n",
    "    bordercolor=\"black\",\n",
    "    borderwidth=1,\n",
    "    borderpad=4\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "print(model.summary())\n",
    "fig.write_image('plots/score_vs_citations.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SETUP ---\n",
    "domain_columns = [f'cites_{d}' for d in DOMAIN_CATEGORIES if d != 'other']\n",
    "\n",
    "model_family_order = ['Perplexity', 'Google', 'OpenAI']\n",
    "domain_order = ['youtube', 'social media', 'community blog', 'tech coding', 'academic journal', 'us news', 'foreign news', 'wiki', 'gov edu', 'retail', 'other']\n",
    "color_map = {\n",
    "    'Google': '#e74c3c',\n",
    "    'Perplexity': '#268bd2',\n",
    "    'OpenAI': '#2ca02c'\n",
    "}\n",
    "\n",
    "# --- Step 0: Filter out data with no citations ---\n",
    "model_data = model_data[model_data[domain_columns].sum(axis=1) != 0]\n",
    "\n",
    "# --- STEP 1: Melt to long format for easier plotting ---\n",
    "melted = model_data.melt(\n",
    "    id_vars=[\"model_family\"],\n",
    "    value_vars=domain_columns,\n",
    "    var_name=\"domain\",\n",
    "    value_name=\"cited\"\n",
    ")\n",
    "\n",
    "# Clean domain names for xticks\n",
    "melted[\"domain\"] = melted[\"domain\"].str.replace(\"cites_\", \"\").str.replace(\"_\", \" \")\n",
    "\n",
    "# --- STEP 2: Group and compute proportion + CI ---\n",
    "summary = (\n",
    "    melted.groupby([\"domain\", \"model_family\"])\n",
    "    .agg(\n",
    "        prop_cited=(\"cited\", \"mean\"),\n",
    "        n=(\"cited\", \"count\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Standard error for proportion: sqrt(p*(1-p)/n), 95% CI via t-score\n",
    "summary[\"se\"] = (summary[\"prop_cited\"] * (1 - summary[\"prop_cited\"]) / summary[\"n\"])**0.5\n",
    "summary[\"t_score\"] = t.ppf(0.975, df=summary[\"n\"] - 1)\n",
    "summary[\"ci\"] = summary[\"t_score\"] * summary[\"se\"]\n",
    "\n",
    "# Preserve domain order and model family order\n",
    "summary[\"domain\"] = pd.Categorical(summary[\"domain\"], categories=domain_order, ordered=True)\n",
    "summary[\"model_family\"] = pd.Categorical(summary[\"model_family\"], categories=model_family_order, ordered=True)\n",
    "summary.sort_values(by=['domain', 'model_family'], inplace=True)\n",
    "\n",
    "# --- STEP 3: Plot ---\n",
    "fig = px.bar(\n",
    "    summary,\n",
    "    x=\"domain\",\n",
    "    y=\"prop_cited\",\n",
    "    color=\"model_family\",\n",
    "    barmode=\"group\",\n",
    "    error_y=\"ci\",\n",
    "    color_discrete_map=color_map,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    xaxis=dict(\n",
    "        title='',\n",
    "        tickfont=dict(size=18, color='black'),\n",
    "        tickangle=-45,\n",
    "        zeroline=False,\n",
    "        linecolor='black',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Proportion of Responses',\n",
    "        title_font=dict(size=20, color='black'),\n",
    "        tickfont=dict(size=16, color='black'),\n",
    "        tickvals=[0, 0.1, 0.2, 0.3, 0.4],\n",
    "        gridcolor='lightgray',\n",
    "        zeroline=False,\n",
    "        linecolor='black',\n",
    "    ),\n",
    "    legend=dict(\n",
    "        title='',\n",
    "        font=dict(size=18, color='black'),\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=0.9,\n",
    "        xanchor='right',\n",
    "        x=1\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    margin=dict(l=0, r=10, t=10, b=0)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# fig.write_image('plots/domain_citations.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['sonar-pro-high', 'sonar-reasoning-pro-high']\n",
    "# --- SETUP ---\n",
    "domain_columns = [f'cites_{d}' for d in DOMAIN_CATEGORIES if d != 'other']\n",
    "\n",
    "# Only keep the models you care about\n",
    "model_data = model_data[model_data[\"model\"].isin(MODELS)]\n",
    "\n",
    "# Order for domains (same as before)\n",
    "domain_order = ['youtube', 'social media', 'community blog', 'tech coding', 'academic journal',\n",
    "                'us news', 'foreign news', 'wiki', 'gov edu', 'retail', 'other']\n",
    "\n",
    "# Optional: specify model order as the MODELS list\n",
    "model_order = MODELS\n",
    "\n",
    "# (Optional) define a color map for specific models if you want\n",
    "# Otherwise you can drop color_discrete_map in px.bar and let Plotly choose\n",
    "color_map = {\n",
    "    'sonar-pro-high': '#47c4ff',\n",
    "    'sonar-reasoning-pro-high': '#3669c3',\n",
    "}\n",
    "\n",
    "# --- Step 0: Filter out data with no citations ---\n",
    "model_data = model_data[model_data[domain_columns].sum(axis=1) != 0]\n",
    "\n",
    "# --- STEP 1: Melt to long format for easier plotting ---\n",
    "melted = model_data.melt(\n",
    "    id_vars=[\"model\"],\n",
    "    value_vars=domain_columns,\n",
    "    var_name=\"domain\",\n",
    "    value_name=\"cited\"\n",
    ")\n",
    "\n",
    "# Clean domain names for xticks\n",
    "melted[\"domain\"] = (\n",
    "    melted[\"domain\"]\n",
    "    .str.replace(\"cites_\", \"\", regex=False)\n",
    "    .str.replace(\"_\", \" \", regex=False)\n",
    ")\n",
    "\n",
    "# --- STEP 2: Group and compute proportion + CI ---\n",
    "summary = (\n",
    "    melted.groupby([\"domain\", \"model\"])   # <-- group by model, not model_family\n",
    "    .agg(\n",
    "        prop_cited=(\"cited\", \"mean\"),\n",
    "        n=(\"cited\", \"count\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Standard error for proportion: sqrt(p*(1-p)/n), 95% CI via t-score\n",
    "summary[\"se\"] = (summary[\"prop_cited\"] * (1 - summary[\"prop_cited\"]) / summary[\"n\"])**0.5\n",
    "summary[\"t_score\"] = t.ppf(0.975, df=summary[\"n\"] - 1)\n",
    "summary[\"ci\"] = summary[\"t_score\"] * summary[\"se\"]\n",
    "\n",
    "# Preserve domain order and model order\n",
    "summary[\"domain\"] = pd.Categorical(summary[\"domain\"], categories=domain_order, ordered=True)\n",
    "summary[\"model\"] = pd.Categorical(summary[\"model\"], categories=model_order, ordered=True)\n",
    "summary.sort_values(by=['domain', 'model'], inplace=True)\n",
    "\n",
    "# --- STEP 3: Plot ---\n",
    "fig = px.bar(\n",
    "    summary,\n",
    "    x=\"domain\",\n",
    "    y=\"prop_cited\",\n",
    "    color=\"model\",\n",
    "    barmode=\"group\",\n",
    "    error_y=\"ci\",\n",
    "    color_discrete_map=color_map,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    xaxis=dict(\n",
    "        title='',\n",
    "        tickfont=dict(size=18, color='black'),\n",
    "        tickangle=-45,\n",
    "        zeroline=False,\n",
    "        linecolor='black',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Proportion of Responses',\n",
    "        title_font=dict(size=20, color='black'),\n",
    "        tickfont=dict(size=16, color='black'),\n",
    "        tickvals=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "        gridcolor='lightgray',\n",
    "        zeroline=False,\n",
    "        linecolor='black',\n",
    "    ),\n",
    "    legend=dict(\n",
    "        title='',\n",
    "        font=dict(size=18, color='black'),\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=0.9,\n",
    "        xanchor='right',\n",
    "        x=1\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    margin=dict(l=0, r=10, t=10, b=0)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image('plots/domain_citations_reasoning_vs_non_reasoning.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted['model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_data_a = pd.DataFrame({'intent': battle_data['primary_intent'],\n",
    "                             'response_len': battle_data['conv_metadata'].apply(lambda x: x['response_length_a']),\n",
    "                             'citation_count': battle_data['conv_metadata'].apply(lambda x: x['citation_count_a'])\n",
    "})\n",
    "intent_data_b = pd.DataFrame({'intent': battle_data['primary_intent'],\n",
    "                             'response_len': battle_data['conv_metadata'].apply(lambda x: x['response_length_b']),\n",
    "                             'citation_count': battle_data['conv_metadata'].apply(lambda x: x['citation_count_b'])\n",
    "})\n",
    "intent_data = pd.concat([intent_data_a, intent_data_b])\n",
    "intent_data = intent_data[intent_data['intent']!='Other']\n",
    "\n",
    "color_map = {\n",
    "    \"Factual Lookup\": \"#bebada\",\n",
    "    \"Info Synthesis\": \"#ffffb3\",\n",
    "    \"Analysis\": \"#8dd3c7\",\n",
    "    \"Recommendation\": \"#80b1d3\",\n",
    "    \"Explanation\": \"#fb8072\",\n",
    "    \"Creative Generation\": \"#fdb462\",\n",
    "    \"Guidance\": \"#b3de69\",\n",
    "    \"Text Processing\": \"#e78ac3\",\n",
    "}\n",
    "\n",
    "# === STEP 1: Compute group stats ===\n",
    "group_stats = intent_data.groupby(\"intent\").agg(\n",
    "    mean_response_len=('response_len', 'mean'),\n",
    "    se_response_len=('response_len', sem),\n",
    "    mean_citation_count=('citation_count', 'mean'),\n",
    "    se_citation_count=('citation_count', sem),\n",
    "    count=('response_len', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# === STEP 2: Compute 95% CI\n",
    "group_stats['t_score'] = t.ppf(0.975, df=group_stats['count'] - 1)\n",
    "group_stats['ci_response_len'] = group_stats['t_score'] * group_stats['se_response_len']\n",
    "group_stats['ci_citation_count'] = group_stats['t_score'] * group_stats['se_citation_count']\n",
    "\n",
    "# === STEP 3: Sort independently for each plot ===\n",
    "sorted_len = group_stats.sort_values(by='mean_response_len', ascending=True).copy()\n",
    "sorted_len['intent'] = pd.Categorical(sorted_len['intent'], categories=sorted_len['intent'], ordered=True)\n",
    "\n",
    "sorted_cit = group_stats.sort_values(by='mean_citation_count', ascending=True).copy()\n",
    "sorted_cit['intent'] = pd.Categorical(sorted_cit['intent'], categories=sorted_cit['intent'], ordered=True)\n",
    "\n",
    "# === STEP 4: Add horizontal bars ===\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    shared_yaxes=False,\n",
    "    horizontal_spacing=0.13,\n",
    ")\n",
    "\n",
    "# Left: Response Length\n",
    "for i, row in sorted_len.iterrows():\n",
    "    color = color_map[row['intent']]\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[row['mean_response_len']],\n",
    "            y=[row['intent']],\n",
    "            orientation='h',\n",
    "            marker_color=color,\n",
    "            error_x=dict(type='data', array=[row['ci_response_len']]),\n",
    "            name=row['intent'],\n",
    "            showlegend=False,\n",
    "            opacity=0.9\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Right: Citation Count\n",
    "for i, row in sorted_cit.iterrows():\n",
    "    color = color_map[row['intent']]\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[row['mean_citation_count']],\n",
    "            y=[row['intent']],\n",
    "            orientation='h',\n",
    "            marker_color=color,\n",
    "            error_x=dict(type='data', array=[row['ci_citation_count']]),\n",
    "            name=row['intent'],\n",
    "            showlegend=False,\n",
    "            opacity=0.9\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# === STEP 5: Layout styling ===\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    height=500,\n",
    "    width=1500,\n",
    "    margin=dict(l=10, r=10, t=30, b=10),\n",
    "    font=dict(size=14, color='black'),\n",
    ")\n",
    "\n",
    "# Y-axis styling\n",
    "fig.update_yaxes(\n",
    "    tickfont=dict(size=20, color='black'),\n",
    "    linecolor='black',\n",
    "    showgrid=False,\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    tickfont=dict(size=18, color='black'),\n",
    "    linecolor='black',\n",
    "    showgrid=False,\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# X-axis: response length\n",
    "fig.update_xaxes(\n",
    "    title='Response Length (words)',\n",
    "    title_font=dict(size=20, color='black'),\n",
    "    tickfont=dict(size=16, color='black'),\n",
    "    linecolor='black',\n",
    "    gridcolor='lightgray',\n",
    "    zeroline=False,\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# X-axis: citation count\n",
    "fig.update_xaxes(\n",
    "    title='Citation Count',\n",
    "    title_font=dict(size=20, color='black'),\n",
    "    tickfont=dict(size=16, color='black'),\n",
    "    linecolor='black',\n",
    "    gridcolor='lightgray',\n",
    "    zeroline=False,\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"plots/length_citation_count_intent.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled BT Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bootstrap_style_coefs(style_coef_bootstrap, style_elements):\n",
    "    lower = np.percentile(style_coef_bootstrap, 2.5, axis=0)\n",
    "    upper = np.percentile(style_coef_bootstrap, 97.5, axis=0)\n",
    "    estimate = np.mean(style_coef_bootstrap, axis=0)\n",
    "    style_element_names = [s[:-2] for s in style_elements[:(len(style_elements)//2)]]\n",
    "    bars = pd.DataFrame({\n",
    "        \"model\": style_element_names,\n",
    "        \"lower\": lower,\n",
    "        \"upper\": upper,\n",
    "        \"rating\": estimate\n",
    "    })\n",
    "    bars[\"error_y\"] = bars[\"upper\"] - bars[\"rating\"]\n",
    "    bars[\"error_y_minus\"] = bars[\"rating\"] - bars[\"lower\"]\n",
    "    bars[\"rating_rounded\"] = np.round(bars[\"rating\"], 2)\n",
    "    bars = bars.sort_values(\"rating\", ascending=False)\n",
    "\n",
    "    fig = px.scatter(\n",
    "        bars,\n",
    "        x=\"model\",\n",
    "        y=\"rating\",\n",
    "        error_y=\"error_y\",\n",
    "        error_y_minus=\"error_y_minus\",\n",
    "        text=\"rating_rounded\",\n",
    "    )\n",
    "    fig.add_hline(y=0, line_width=1, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=10, color=\"royalblue\"),\n",
    "        line=dict(width=1, color=\"royalblue\"),\n",
    "        textposition='top left',\n",
    "        textfont=dict(size=16, color='black')\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(\n",
    "            title='',\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            linecolor='black',\n",
    "            linewidth=1.0,\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(text='Coefficient Estimate', font=dict(size=18, color='black')),\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            range=[min(-0.1, lower.min()-0.1), upper.max()+0.1],\n",
    "            linecolor='black',\n",
    "            linewidth=1.0,\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray'\n",
    "        ),\n",
    "        margin=dict(l=10, r=10, t=10, b=10),\n",
    "        height=500,\n",
    "        width=700,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def visualize_single_style_coef_across_subsets(subset_dict, style_elements, anchor_model='gpt-4o-search', num_bootstrap_samples=100):\n",
    "    assert len(style_elements) == 2\n",
    "    records = []\n",
    "    feature_name = style_elements[0][:-2]\n",
    "\n",
    "    for subset_name, df in subset_dict.items():\n",
    "        _, style_coef_bootstrap = get_bt_ratings(\n",
    "            df,\n",
    "            anchor_model=anchor_model,\n",
    "            num_bootstrap_samples=num_bootstrap_samples,\n",
    "            style_elements=style_elements\n",
    "        )\n",
    "\n",
    "        estimate = np.mean(style_coef_bootstrap, axis=0)[0]\n",
    "        lower = np.percentile(style_coef_bootstrap, 2.5, axis=0)[0]\n",
    "        upper = np.percentile(style_coef_bootstrap, 97.5, axis=0)[0]\n",
    "\n",
    "        records.append({\n",
    "            \"Subset\": subset_name,\n",
    "            \"Estimate\": estimate,\n",
    "            \"Lower\": lower,\n",
    "            \"Upper\": upper,\n",
    "            \"Text\": f\"{estimate:.2f}\",\n",
    "        })\n",
    "\n",
    "    df_plot = pd.DataFrame(records)\n",
    "    df_plot[\"Error Plus\"] = df_plot[\"Upper\"] - df_plot[\"Estimate\"]\n",
    "    df_plot[\"Error Minus\"] = df_plot[\"Estimate\"] - df_plot[\"Lower\"]\n",
    "    df_plot = df_plot.sort_values(by='Estimate')\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df_plot,\n",
    "        x=\"Subset\",\n",
    "        y=\"Estimate\",\n",
    "        error_y=\"Error Plus\",\n",
    "        error_y_minus=\"Error Minus\",\n",
    "        text=\"Text\"\n",
    "    )\n",
    "\n",
    "    fig.add_hline(y=0, line_width=1, line_dash=\"dash\", line_color=\"gray\")\n",
    "\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=10, color=\"royalblue\"),\n",
    "        textposition='top right',\n",
    "        textfont=dict(size=14, color='black')\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        showlegend=False,\n",
    "        xaxis=dict(\n",
    "            title='',\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            linecolor='black',\n",
    "            linewidth=1.0,\n",
    "            showgrid=False\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(text='Coefficient Estimate', font=dict(size=18, color='black')),\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            linecolor='black',\n",
    "            linewidth=1.0,\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray'\n",
    "        ),\n",
    "        margin=dict(l=30, r=30, t=10, b=10),\n",
    "        height=500,\n",
    "        width=800,\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_elements = ['response_length_a', 'response_length_b']\n",
    "bt_ratings_bootstrap, style_coef_bootstrap = get_bt_ratings(battle_data, anchor_model='gpt-4o-search', num_bootstrap_samples=100, style_elements=style_elements)\n",
    "fig = visualize_bootstrap_style_coefs(style_coef_bootstrap, style_elements)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_elements = ['response_length_a', 'response_length_b']\n",
    "subsets = {primary_intent: battle_data[battle_data['primary_intent']==primary_intent] for primary_intent in battle_data['primary_intent'].unique() if primary_intent != 'Other'}\n",
    "fig = visualize_single_style_coef_across_subsets(subsets, style_elements)\n",
    "fig.show()\n",
    "fig.write_image(\"plots/response_length_intent.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citation count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_data = battle_data[battle_data['conv_metadata'].apply(lambda x: x['citation_count_a'] > 0 and x['citation_count_b'] > 0)]\n",
    "style_elements = [\"citation_count_a\", \"citation_count_b\"]\n",
    "bt_ratings_bootstrap, style_coef_bootstrap = get_bt_ratings(curr_data, anchor_model='gpt-4o-search', num_bootstrap_samples=100, style_elements=style_elements)\n",
    "fig = visualize_bootstrap_style_coefs(style_coef_bootstrap, style_elements)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_data = battle_data[battle_data['conv_metadata'].apply(lambda x: x['citation_count_a'] > 0 and x['citation_count_b'] > 0)]\n",
    "style_elements = ['citation_count_a', 'citation_count_b']\n",
    "subsets = {primary_intent: battle_data[battle_data['primary_intent']==primary_intent] for primary_intent in battle_data['primary_intent'].unique() if primary_intent != 'Other'}\n",
    "fig = visualize_single_style_coef_across_subsets(subsets, style_elements)\n",
    "fig.show()\n",
    "fig.write_image(\"plots/citation_count_intent.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citation sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_battle_data = battle_data[battle_data['conv_metadata'].apply(lambda x: x['citation_count_a'] > 0 and x['citation_count_b'] > 0)]\n",
    "curr_battle_data = curr_battle_data[curr_battle_data['winner'].isin(['model_a', 'model_b'])]\n",
    "\n",
    "DOMAIN_CATEGORIES = [\n",
    "    \"youtube\",\n",
    "    \"gov_edu\",\n",
    "    \"wiki\",\n",
    "    \"us_news\",\n",
    "    \"foreign_news\",\n",
    "    \"social_media\",\n",
    "    \"community_blog\",\n",
    "    \"tech_coding\",\n",
    "    \"academic_journal\",\n",
    "    \"retail\",\n",
    "    \"other\"\n",
    "]\n",
    "\n",
    "# --- Step 1: Compute control coefficients ---\n",
    "CONTROL_ELEMENTS = [\"citation_count_a\"] + [f\"cites_{domain}_a\" for domain in DOMAIN_CATEGORIES]\n",
    "CONTROL_ELEMENTS += [\"citation_count_b\"] + [f\"cites_{domain}_b\" for domain in DOMAIN_CATEGORIES]\n",
    "anchor_model = 'gpt-4o-search'\n",
    "anchor_rating = 1000\n",
    "bt_ratings, _ = compute_style_control(curr_battle_data, style_elements=CONTROL_ELEMENTS)\n",
    "offset_score = (anchor_rating - bt_ratings[anchor_model])\n",
    "bt_ratings += offset_score\n",
    "bt_ratings_bootstrap, coef_bootstrap = compute_bootstrap_style_control(curr_battle_data, style_elements=CONTROL_ELEMENTS, num_round=100, offset=offset_score)\n",
    "coef_names = [s[:-2] for s in CONTROL_ELEMENTS[:(len(CONTROL_ELEMENTS)//2)]]\n",
    "\n",
    "# --- Step 2: Compute CI and mean ---\n",
    "lower = np.percentile(coef_bootstrap, 2.5, axis=0)\n",
    "upper = np.percentile(coef_bootstrap, 97.5, axis=0)\n",
    "mean = np.mean(coef_bootstrap, axis=0)\n",
    "coef_df = pd.DataFrame({\n",
    "    'coef_name': coef_names,\n",
    "    'mean': mean,\n",
    "    'ci_lower': lower,\n",
    "    'ci_upper': upper,\n",
    "    'ci_width': (upper - lower) / 2\n",
    "})\n",
    "coef_df = coef_df.sort_values(by='mean', ascending=True)\n",
    "coef_df['coef_name'] = coef_df['coef_name'].str.replace(\"cites_\", \"\").str.replace(\"_\", \" \")\n",
    "coef_df = coef_df[(coef_df['coef_name'] != 'other')]\n",
    "\n",
    "def get_significance_color(row):\n",
    "    if row['ci_lower'] > 0:\n",
    "        return '#2ca02c'\n",
    "    elif row['ci_upper'] < 0:\n",
    "        return '#e74c3c'\n",
    "    else:\n",
    "        return '#a9a9a9'\n",
    "coef_df['color'] = coef_df.apply(get_significance_color, axis=1)\n",
    "\n",
    "# --- Step 3: Plot ---\n",
    "fig = px.scatter(\n",
    "    coef_df,\n",
    "    x='mean',\n",
    "    y='coef_name',\n",
    "    error_x='ci_width',\n",
    "    error_x_minus='ci_width',\n",
    "    color=coef_df['color'],\n",
    "    color_discrete_map='identity'\n",
    ")\n",
    "fig.update_traces(\n",
    "    marker=dict(size=14, symbol='circle'),\n",
    "    error_x=dict(thickness=2.5),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=0,\n",
    "    line=dict(color='black', width=1.5, dash='dash')\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    title='',\n",
    "    xaxis=dict(\n",
    "        title=dict(text='Coefficient Estimate', font=dict(size=18, color='black')),\n",
    "        linecolor='black',\n",
    "        tickfont=dict(size=16, color='black'),\n",
    "        gridcolor='lightgray',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='',\n",
    "        tickfont=dict(size=18, color='black'),\n",
    "        linecolor='black',\n",
    "    ),\n",
    "    font=dict(size=14),\n",
    "    width=800,\n",
    "    height=500,\n",
    "    margin=dict(l=10, r=10, t=20, b=20)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image('plots/citation_coefs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_ELEMENTS = [\"response_length_a\", \"citation_count_a\"] + [f\"cites_{domain}_a\" for domain in DOMAIN_CATEGORIES]\n",
    "CONTROL_ELEMENTS += [\"response_length_b\", \"citation_count_b\"] + [f\"cites_{domain}_b\" for domain in DOMAIN_CATEGORIES]\n",
    "\n",
    "anchor_model = 'gpt-4o-search'\n",
    "bt_ratings = compute_bt(battle_data)\n",
    "offset_score = (anchor_rating - bt_ratings[anchor_model])\n",
    "bt_ratings += offset_score\n",
    "\n",
    "bt_ratings_style, _ = compute_style_control(curr_battle_data, style_elements=CONTROL_ELEMENTS)\n",
    "offset_score = (anchor_rating - bt_ratings_style[anchor_model])\n",
    "bt_ratings_style += offset_score\n",
    "\n",
    "bt_change_scores = {}\n",
    "for model in get_model_order(curr_battle_data):\n",
    "    if model in [\"gpt-4o-mini-search\", \"gpt-4o-search-high-loc\", \"sonar-pro\"]:\n",
    "        continue\n",
    "    random.seed(42)\n",
    "    bt_change_scores[model] = (bt_ratings[model] + random.uniform(-2.0, 2.0), bt_ratings_style[model] + random.uniform(-2.0, 2.0))\n",
    "    \n",
    "def viz_change_scores(bt_change_scores, initial_name, final_name):\n",
    "    full_palettes = {\n",
    "        \"pplx\": px.colors.sequential.Blues,\n",
    "        \"gemini\": px.colors.sequential.Oranges,\n",
    "        \"openai\": px.colors.sequential.Greens,\n",
    "    }\n",
    "    families = {\n",
    "        \"pplx\": sorted(m for m in bt_change_scores if m.startswith(\"sonar\")),\n",
    "        \"gemini\": sorted(m for m in bt_change_scores if m.startswith(\"gemini\")),\n",
    "        \"openai\": sorted(m for m in bt_change_scores if \"gpt-4o\" in m),\n",
    "    }\n",
    "\n",
    "    color_mapping = {}\n",
    "    for fam, models in families.items():\n",
    "        palette = full_palettes[fam]\n",
    "        L = len(palette)\n",
    "        start = int(L * 0.3)\n",
    "        end   = int(L * 0.9)\n",
    "        trimmed = palette[start:end] or palette\n",
    "        n = len(models)\n",
    "        idxs = np.linspace(0, len(trimmed) - 1, n, dtype=int)\n",
    "        for model, idx in zip(models, idxs):\n",
    "            color_mapping[model] = trimmed[idx]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for model, (score0, score1) in bt_change_scores.items():\n",
    "        fam = next((f for f, mods in families.items() if model in mods), None)\n",
    "        col = color_mapping.get(model, \"#444444\")\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[initial_name, final_name],\n",
    "            y=[score0, score1],\n",
    "            mode='lines+markers',\n",
    "            name=model,\n",
    "            legendgroup=fam,\n",
    "            marker=dict(size=10, color=col),\n",
    "            line=dict(width=3, color=col),\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        yaxis_title=\"Arena Score\",\n",
    "        height=600, width=800,\n",
    "        plot_bgcolor=\"white\",\n",
    "        margin=dict(t=20, b=20, l=20, r=20),\n",
    "        xaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridcolor='lightgray',\n",
    "            tickfont=dict(size=20, color='black'),\n",
    "            linecolor='black',\n",
    "            linewidth=1.0\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            title_font=dict(size=18, color='black'),\n",
    "            showgrid=True, \n",
    "            gridcolor='lightgray',\n",
    "            linecolor='black',\n",
    "            linewidth=1.0,\n",
    "            range=[975, 1170],\n",
    "            tickmode='linear',\n",
    "            tick0=1000,\n",
    "            dtick=25\n",
    "        ),\n",
    "        legend=dict(traceorder=\"grouped\", font=dict(size=15, color='black'))\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "fig = viz_change_scores(bt_change_scores, \"original\", \"controlled\")\n",
    "fig.show()\n",
    "fig.write_image(\"plots/style_control_ranking.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation attribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battle_data = pd.read_json('../data/citation_attribution_data.jsonl', orient='records', lines=True)\n",
    "battle_data['timestamp'] = pd.to_datetime(battle_data['timestamp']).dt.tz_localize('UTC').dt.tz_convert('America/Los_Angeles')\n",
    "battle_data = battle_data[battle_data['winner'].notna()].reset_index(drop=True)\n",
    "battle_data.drop(columns=['conv_metadata'], inplace=True)\n",
    "\n",
    "model_mapping = {\n",
    "    \"gpt-4o-mini-search-preview\": \"gpt-4o-mini-search\",\n",
    "    \"gpt-4o-mini-search-preview-high\": \"gpt-4o-mini-search-high\",\n",
    "    \"gpt-4o-search-preview\": \"gpt-4o-search\",\n",
    "    \"gpt-4o-search-preview-high\": \"gpt-4o-search-high\",\n",
    "    \"gpt-4o-search-preview-high-loc\": \"gpt-4o-search-high-loc\",\n",
    "    \"gemini-2.0-flash-grounding\": \"gemini-2.0-flash-grounding\",\n",
    "    \"gemini-2.5-flash-preview-04-17-grounding\": \"gemini-2.5-flash-grounding\",\n",
    "    \"gemini-2.0-pro-exp-02-05-grounding\": \"gemini-2.5-pro-grounding\",\n",
    "    \"gemini-2.5-pro-exp-03-25-grounding\": \"gemini-2.5-pro-grounding\",\n",
    "    \"gemini-2.5-pro-exp-03-25-wo-search\": \"gemini-2.5-pro\"\n",
    "}\n",
    "battle_data[\"model_a\"].replace(model_mapping, inplace=True)\n",
    "battle_data[\"model_b\"].replace(model_mapping, inplace=True)\n",
    "\n",
    "add_num_citations_style(battle_data, \"citation_count\")\n",
    "add_cit_misattribution_counts(battle_data)\n",
    "\n",
    "print(f'Number of battles with votes: {len(battle_data)}')\n",
    "display(battle_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_models = [\"sonar-reasoning\", \"sonar-reasoning-pro-high\"]\n",
    "non_reasoning_models = [\"sonar\", \"sonar-pro\"]\n",
    "reasoning_support, reasoning_irrelevant, reasoning_contradict = [], [], []\n",
    "non_reasoning_support, non_reasoning_irrelevant, non_reasoning_contradict = [], [], []\n",
    "for _, row in battle_data.iterrows():\n",
    "    if row[\"model_a\"] in reasoning_models:\n",
    "        reasoning_support.append(row[\"conv_metadata\"][\"support_count_a\"] / (row[\"conv_metadata\"][\"support_count_a\"] + row[\"conv_metadata\"][\"irrelevant_count_a\"] + row[\"conv_metadata\"][\"contradict_count_a\"]))\n",
    "        reasoning_irrelevant.append(row[\"conv_metadata\"][\"irrelevant_count_a\"] / (row[\"conv_metadata\"][\"support_count_a\"] + row[\"conv_metadata\"][\"irrelevant_count_a\"] + row[\"conv_metadata\"][\"contradict_count_a\"]))\n",
    "        reasoning_contradict.append(row[\"conv_metadata\"][\"contradict_count_a\"] / (row[\"conv_metadata\"][\"support_count_a\"] + row[\"conv_metadata\"][\"irrelevant_count_a\"] + row[\"conv_metadata\"][\"contradict_count_a\"]))\n",
    "    else:\n",
    "        non_reasoning_support.append(row[\"conv_metadata\"][\"support_count_a\"] / (row[\"conv_metadata\"][\"support_count_a\"] + row[\"conv_metadata\"][\"irrelevant_count_a\"] + row[\"conv_metadata\"][\"contradict_count_a\"]))\n",
    "        non_reasoning_irrelevant.append(row[\"conv_metadata\"][\"irrelevant_count_a\"] / (row[\"conv_metadata\"][\"support_count_a\"] + row[\"conv_metadata\"][\"irrelevant_count_a\"] + row[\"conv_metadata\"][\"contradict_count_a\"]))\n",
    "        non_reasoning_contradict.append(row[\"conv_metadata\"][\"contradict_count_a\"] / (row[\"conv_metadata\"][\"support_count_a\"] + row[\"conv_metadata\"][\"irrelevant_count_a\"] + row[\"conv_metadata\"][\"contradict_count_a\"]))\n",
    "    if row[\"model_b\"] in reasoning_models:\n",
    "        reasoning_support.append(row[\"conv_metadata\"][\"support_count_b\"] / (row[\"conv_metadata\"][\"support_count_b\"] + row[\"conv_metadata\"][\"irrelevant_count_b\"] + row[\"conv_metadata\"][\"contradict_count_b\"]))\n",
    "        reasoning_irrelevant.append(row[\"conv_metadata\"][\"irrelevant_count_b\"] / (row[\"conv_metadata\"][\"support_count_b\"] + row[\"conv_metadata\"][\"irrelevant_count_b\"] + row[\"conv_metadata\"][\"contradict_count_b\"]))\n",
    "        reasoning_contradict.append(row[\"conv_metadata\"][\"contradict_count_b\"] / (row[\"conv_metadata\"][\"support_count_b\"] + row[\"conv_metadata\"][\"irrelevant_count_b\"] + row[\"conv_metadata\"][\"contradict_count_b\"]))\n",
    "    else:\n",
    "        non_reasoning_support.append(row[\"conv_metadata\"][\"support_count_b\"] / (row[\"conv_metadata\"][\"support_count_b\"] + row[\"conv_metadata\"][\"irrelevant_count_b\"] + row[\"conv_metadata\"][\"contradict_count_b\"]))\n",
    "        non_reasoning_irrelevant.append(row[\"conv_metadata\"][\"irrelevant_count_b\"] / (row[\"conv_metadata\"][\"support_count_b\"] + row[\"conv_metadata\"][\"irrelevant_count_b\"] + row[\"conv_metadata\"][\"contradict_count_b\"]))\n",
    "        non_reasoning_contradict.append(row[\"conv_metadata\"][\"contradict_count_b\"] / (row[\"conv_metadata\"][\"support_count_b\"] + row[\"conv_metadata\"][\"irrelevant_count_b\"] + row[\"conv_metadata\"][\"contradict_count_b\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmean(reasoning_support))\n",
    "print(np.nanmean(non_reasoning_support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmean(reasoning_irrelevant))\n",
    "print(np.nanmean(non_reasoning_irrelevant))\n",
    "print(np.nanmean(reasoning_contradict))\n",
    "print(np.nanmean(non_reasoning_contradict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_remove = [\"gemini-2.5-pro\"]\n",
    "curr_battle_data = battle_data[~battle_data[\"model_a\"].isin(models_to_remove) & ~battle_data[\"model_b\"].isin(models_to_remove)]\n",
    "curr_battle_data = curr_battle_data[curr_battle_data['conv_metadata'].apply(lambda x: x['citation_count_a'] > 0 and x['citation_count_b'] > 0)]\n",
    "curr_battle_data = curr_battle_data[curr_battle_data['winner'].isin(['model_a', 'model_b'])]\n",
    "\n",
    "# --- Step 1: Compute control coefficients ---\n",
    "CONTROL_ELEMENTS = [\"support_count_a\", \"irrelevant_count_a\", \"contradict_count_a\"]\n",
    "CONTROL_ELEMENTS += [\"support_count_b\", \"irrelevant_count_b\", \"contradict_count_b\"]\n",
    "anchor_model = 'gpt-4o-search'\n",
    "anchor_rating = 1000\n",
    "bt_ratings, _ = compute_style_control(curr_battle_data, style_elements=CONTROL_ELEMENTS)\n",
    "offset_score = (anchor_rating - bt_ratings[anchor_model])\n",
    "bt_ratings += offset_score\n",
    "bt_ratings_bootstrap, coef_bootstrap = compute_bootstrap_style_control(curr_battle_data, style_elements=CONTROL_ELEMENTS, num_round=100, offset=offset_score)\n",
    "coef_names = [s[:-2] for s in CONTROL_ELEMENTS[:(len(CONTROL_ELEMENTS)//2)]]\n",
    "\n",
    "# --- Step 2: Compute CI and mean ---\n",
    "lower = np.percentile(coef_bootstrap, 2.5, axis=0)\n",
    "upper = np.percentile(coef_bootstrap, 97.5, axis=0)\n",
    "mean = np.mean(coef_bootstrap, axis=0)\n",
    "coef_df = pd.DataFrame({\n",
    "    'coef_name': coef_names,\n",
    "    'mean': mean,\n",
    "    'ci_lower': lower,\n",
    "    'ci_upper': upper,\n",
    "    'ci_width': (upper - lower) / 2\n",
    "})\n",
    "coef_df = coef_df.sort_values(by='mean', ascending=True)\n",
    "coef_df['coef_name'] = coef_df['coef_name'].str.replace(\"cites_\", \"\").str.replace(\"_\", \" \")\n",
    "coef_df = coef_df[(coef_df['coef_name'] != 'other')]\n",
    "\n",
    "def get_significance_color(row):\n",
    "    if row['ci_lower'] > 0:\n",
    "        return '#2ca02c'\n",
    "    elif row['ci_upper'] < 0:\n",
    "        return '#e74c3c'\n",
    "    else:\n",
    "        return '#a9a9a9'\n",
    "coef_df['color'] = coef_df.apply(get_significance_color, axis=1)\n",
    "\n",
    "# --- Step 3: Plot ---\n",
    "fig = px.scatter(\n",
    "    coef_df,\n",
    "    x='mean',\n",
    "    y='coef_name',\n",
    "    error_x='ci_width',\n",
    "    error_x_minus='ci_width',\n",
    "    color=coef_df['color'],\n",
    "    color_discrete_map='identity'\n",
    ")\n",
    "fig.update_traces(\n",
    "    marker=dict(size=14, symbol='circle'),\n",
    "    error_x=dict(thickness=2.5),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=0,\n",
    "    line=dict(color='black', width=1.5, dash='dash')\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    title='',\n",
    "    xaxis=dict(\n",
    "        title=dict(text='Coefficient Estimate', font=dict(size=18, color='black')),\n",
    "        linecolor='black',\n",
    "        tickfont=dict(size=16, color='black'),\n",
    "        gridcolor='lightgray',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='',\n",
    "        tickfont=dict(size=18, color='black'),\n",
    "        linecolor='black',\n",
    "    ),\n",
    "    font=dict(size=14),\n",
    "    width=450,\n",
    "    height=500,\n",
    "    margin=dict(l=0, r=0, t=5, b=5)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# fig.write_image('plots/misattribution_coefs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (curr_battle_data[\"winner\"] == \"model_a\").astype(int)\n",
    "X = curr_battle_data[\"conv_metadata\"].apply(lambda x: x[\"support_count_a\"] - x[\"support_count_b\"])\n",
    "r_hat = np.corrcoef(X, Y)[0, 1]\n",
    "N = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Helper: Fisher z transform and inverse ---\n",
    "def fisher_z(r: float) -> float:\n",
    "    \"\"\"Fisher z-transform of a correlation.\"\"\"\n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "def fisher_z_inv(z: float) -> float:\n",
    "    \"\"\"Inverse Fisher z-transform.\"\"\"\n",
    "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "\n",
    "# --- Power for testing correlation != 0 ---\n",
    "def power_for_correlation(\n",
    "    N: int,\n",
    "    rho: float,\n",
    "    alpha: float = 0.05,\n",
    "    two_sided: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Approximate power to detect a true correlation `rho` with sample size N,\n",
    "    using Fisher's z transform and a normal approximation.\n",
    "    \"\"\"\n",
    "    if N <= 3:\n",
    "        raise ValueError(\"N must be > 3\")\n",
    "\n",
    "    z_rho = fisher_z(rho)\n",
    "    se = 1 / np.sqrt(N - 3)\n",
    "\n",
    "    if two_sided:\n",
    "        z_alpha = norm.ppf(1 - alpha / 2)\n",
    "    else:\n",
    "        z_alpha = norm.ppf(1 - alpha)\n",
    "\n",
    "    # Under H1, z_hat ~ Normal(mean = z_rho, sd = se)\n",
    "    # A common approximation for power (rho > 0) is:\n",
    "    # power ≈ Phi( (z_rho / se) - z_alpha )\n",
    "    mu = z_rho / se\n",
    "    power = norm.cdf(mu - z_alpha)\n",
    "    return power\n",
    "\n",
    "# --- Required N for given rho and desired power ---\n",
    "def required_N_for_correlation(\n",
    "    rho: float,\n",
    "    power: float = 0.8,\n",
    "    alpha: float = 0.05,\n",
    "    two_sided: bool = True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Approximate N needed to detect a true correlation `rho` with given power.\n",
    "    \"\"\"\n",
    "    if abs(rho) >= 1:\n",
    "        raise ValueError(\"rho must be in (-1, 1)\")\n",
    "\n",
    "    z_rho = fisher_z(rho)\n",
    "\n",
    "    if two_sided:\n",
    "        z_alpha = norm.ppf(1 - alpha / 2)\n",
    "    else:\n",
    "        z_alpha = norm.ppf(1 - alpha)\n",
    "\n",
    "    z_beta = norm.ppf(power)\n",
    "\n",
    "    N = 3 + ((z_alpha + z_beta) ** 2) / (z_rho ** 2)\n",
    "    return int(np.ceil(N))\n",
    "\n",
    "# --- 95% CI for the observed correlation ---\n",
    "def ci_for_correlation(\n",
    "    r: float,\n",
    "    N: int,\n",
    "    alpha: float = 0.05,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    95% CI for a sample correlation using Fisher's z transform.\n",
    "    \"\"\"\n",
    "    z = fisher_z(r)\n",
    "    se = 1 / np.sqrt(N - 3)\n",
    "    z_alpha = norm.ppf(1 - alpha / 2)\n",
    "    z_lo = z - z_alpha * se\n",
    "    z_hi = z + z_alpha * se\n",
    "    return fisher_z_inv(z_lo), fisher_z_inv(z_hi)\n",
    "\n",
    "# ============================\n",
    "# Example usage with your data\n",
    "# ============================\n",
    "\n",
    "print(f\"Observed correlation r_hat = {r_hat:.3f}, N = {N}\")\n",
    "\n",
    "# 1) CI for the observed correlation\n",
    "ci_lo, ci_hi = ci_for_correlation(r_hat, N)\n",
    "print(f\"95% CI for r_hat: [{ci_lo:.3f}, {ci_hi:.3f}]\")\n",
    "\n",
    "# 2) Retrospective power: assuming true rho = r_hat\n",
    "power_at_r_hat = power_for_correlation(N, r_hat, alpha=0.05, two_sided=True)\n",
    "print(f\"Approx. power at rho = r_hat: {power_at_r_hat:.3f}\")\n",
    "\n",
    "# 3) Prospective power: what N do we need for a target rho?\n",
    "# e.g. rho = 0.10 as a \"meaningful\" effect\n",
    "target_rho = 0.10\n",
    "needed_N_80 = required_N_for_correlation(target_rho, power=0.8, alpha=0.05, two_sided=True)\n",
    "needed_N_90 = required_N_for_correlation(target_rho, power=0.9, alpha=0.05, two_sided=True)\n",
    "\n",
    "print(f\"N needed for rho = {target_rho:.2f}, 80% power: {needed_N_80}\")\n",
    "print(f\"N needed for rho = {target_rho:.2f}, 90% power: {needed_N_90}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- 1. Helper Functions (Math remains the same) ---\n",
    "def fisher_z(r: float) -> float:\n",
    "    \"\"\"Fisher z-transform of a correlation.\"\"\"\n",
    "    # Clip r to avoid inf/nan if r is exactly 1 or -1\n",
    "    r = np.clip(r, -0.999999, 0.999999)\n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "def fisher_z_inv(z: float) -> float:\n",
    "    \"\"\"Inverse Fisher z-transform.\"\"\"\n",
    "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "\n",
    "def get_minimum_detectable_effect(N: int, power: float = 0.8, alpha: float = 0.05) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the smallest correlation (rho) that can be detected\n",
    "    with specific Power and Alpha, given Sample Size N.\n",
    "    \"\"\"\n",
    "    # Rearranging the sample size formula:\n",
    "    # sqrt(N - 3) = (z_alpha + z_beta) / z_rho\n",
    "    # z_rho = (z_alpha + z_beta) / sqrt(N - 3)\n",
    "    \n",
    "    z_alpha = norm.ppf(1 - alpha / 2) # Two-sided\n",
    "    z_beta = norm.ppf(power)\n",
    "    \n",
    "    z_rho_required = (z_alpha + z_beta) / np.sqrt(N - 3)\n",
    "    return fisher_z_inv(z_rho_required)\n",
    "\n",
    "def ci_for_correlation(r: float, N: int, alpha: float = 0.05) -> tuple[float, float]:\n",
    "    \"\"\"95% CI for a sample correlation.\"\"\"\n",
    "    z = fisher_z(r)\n",
    "    se = 1 / np.sqrt(N - 3)\n",
    "    z_alpha = norm.ppf(1 - alpha / 2)\n",
    "    z_lo = z - z_alpha * se\n",
    "    z_hi = z + z_alpha * se\n",
    "    return fisher_z_inv(z_lo), fisher_z_inv(z_hi)\n",
    "\n",
    "# --- 2. Prepare Data ---\n",
    "# Ensure we drop NaNs so N is accurate\n",
    "df_clean = curr_battle_data.dropna(subset=[\"winner\", \"conv_metadata\"])\n",
    "Y = (df_clean[\"winner\"] == \"model_a\").astype(int)\n",
    "# Extract support counts safely\n",
    "X = df_clean[\"conv_metadata\"].apply(lambda x: x.get(\"support_count_a\", 0) - x.get(\"support_count_b\", 0))\n",
    "\n",
    "# --- 3. Run Analysis ---\n",
    "N = len(X)\n",
    "r_hat = np.corrcoef(X, Y)[0, 1]\n",
    "\n",
    "print(f\"--- ANALYSIS RESULTS (N={N}) ---\")\n",
    "print(f\"Observed Correlation (r): {r_hat:.4f}\")\n",
    "\n",
    "# A. Confidence Interval (The \"Precision\" Argument)\n",
    "ci_lo, ci_hi = ci_for_correlation(r_hat, N)\n",
    "print(f\"95% Confidence Interval: [{ci_lo:.4f}, {ci_hi:.4f}]\")\n",
    "\n",
    "# B. Sensitivity Analysis (The \"Robustness\" Argument)\n",
    "# What is the smallest effect we theoretically COULD see?\n",
    "mde_80 = get_minimum_detectable_effect(N, power=0.80)\n",
    "print(f\"\\n--- SENSITIVITY ANALYSIS ---\")\n",
    "print(f\"With N={N}, you have 80% power to detect a correlation as small as r = {mde_80:.4f}\")\n",
    "\n",
    "# C. Contextual Comparison\n",
    "# Cohen's conventions: 0.1 (small), 0.3 (medium), 0.5 (large)\n",
    "print(f\"Can you detect a 'Small' effect (r=0.10)? {'YES' if mde_80 < 0.10 else 'NO'}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# D. Interpretation for Paper\n",
    "if ci_lo > 0:\n",
    "    print(\"CONCLUSION: Significant Positive Correlation.\")\n",
    "    print(f\"We detected an effect of r={r_hat:.3f}. Since our sample N={N} is capable of detecting\")\n",
    "    print(f\"effects as small as r={mde_80:.3f}, this finding is robust.\")\n",
    "elif ci_hi < 0:\n",
    "    print(\"CONCLUSION: Significant Negative Correlation.\")\n",
    "else:\n",
    "    print(\"CONCLUSION: No Significant Correlation detected.\")\n",
    "    if mde_80 < 0.1:\n",
    "        print(\"However, our analysis shows we had Power to detect even a small effect (r=0.1).\")\n",
    "        print(\"Therefore, the lack of significance suggests the true effect is likely negligible, not just missed.\")\n",
    "    else:\n",
    "        print(\"Warning: Our sample size might have been too small to detect weak effects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- 1. Helper Functions (Math remains the same) ---\n",
    "def fisher_z(r: float) -> float:\n",
    "    \"\"\"Fisher z-transform of a correlation.\"\"\"\n",
    "    # Clip r to avoid inf/nan if r is exactly 1 or -1\n",
    "    r = np.clip(r, -0.999999, 0.999999)\n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "def fisher_z_inv(z: float) -> float:\n",
    "    \"\"\"Inverse Fisher z-transform.\"\"\"\n",
    "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "\n",
    "def get_minimum_detectable_effect(N: int, power: float = 0.8, alpha: float = 0.05) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the smallest correlation (rho) that can be detected\n",
    "    with specific Power and Alpha, given Sample Size N.\n",
    "    \"\"\"\n",
    "    # Rearranging the sample size formula:\n",
    "    # sqrt(N - 3) = (z_alpha + z_beta) / z_rho\n",
    "    # z_rho = (z_alpha + z_beta) / sqrt(N - 3)\n",
    "    \n",
    "    z_alpha = norm.ppf(1 - alpha / 2) # Two-sided\n",
    "    z_beta = norm.ppf(power)\n",
    "    \n",
    "    z_rho_required = (z_alpha + z_beta) / np.sqrt(N - 3)\n",
    "    return fisher_z_inv(z_rho_required)\n",
    "\n",
    "def ci_for_correlation(r: float, N: int, alpha: float = 0.05) -> tuple[float, float]:\n",
    "    \"\"\"95% CI for a sample correlation.\"\"\"\n",
    "    z = fisher_z(r)\n",
    "    se = 1 / np.sqrt(N - 3)\n",
    "    z_alpha = norm.ppf(1 - alpha / 2)\n",
    "    z_lo = z - z_alpha * se\n",
    "    z_hi = z + z_alpha * se\n",
    "    return fisher_z_inv(z_lo), fisher_z_inv(z_hi)\n",
    "\n",
    "# --- 2. Prepare Data ---\n",
    "# Ensure we drop NaNs so N is accurate\n",
    "df_clean = curr_battle_data.dropna(subset=[\"winner\", \"conv_metadata\"])\n",
    "Y = (df_clean[\"winner\"] == \"model_a\").astype(int)\n",
    "# Extract support counts safely\n",
    "X = df_clean[\"conv_metadata\"].apply(lambda x: x.get(\"contradict_count_a\", 0) - x.get(\"contradict_count_b\", 0))\n",
    "\n",
    "# --- 3. Run Analysis ---\n",
    "N = len(X)\n",
    "r_hat = np.corrcoef(X, Y)[0, 1]\n",
    "\n",
    "print(f\"--- ANALYSIS RESULTS (N={N}) ---\")\n",
    "print(f\"Observed Correlation (r): {r_hat:.4f}\")\n",
    "\n",
    "# A. Confidence Interval (The \"Precision\" Argument)\n",
    "ci_lo, ci_hi = ci_for_correlation(r_hat, N)\n",
    "print(f\"95% Confidence Interval: [{ci_lo:.4f}, {ci_hi:.4f}]\")\n",
    "\n",
    "# B. Sensitivity Analysis (The \"Robustness\" Argument)\n",
    "# What is the smallest effect we theoretically COULD see?\n",
    "mde_80 = get_minimum_detectable_effect(N, power=0.80)\n",
    "print(f\"\\n--- SENSITIVITY ANALYSIS ---\")\n",
    "print(f\"With N={N}, you have 80% power to detect a correlation as small as r = {mde_80:.4f}\")\n",
    "\n",
    "# C. Contextual Comparison\n",
    "# Cohen's conventions: 0.1 (small), 0.3 (medium), 0.5 (large)\n",
    "print(f\"Can you detect a 'Small' effect (r=0.10)? {'YES' if mde_80 < 0.10 else 'NO'}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# D. Interpretation for Paper\n",
    "if ci_lo > 0:\n",
    "    print(\"CONCLUSION: Significant Positive Correlation.\")\n",
    "    print(f\"We detected an effect of r={r_hat:.3f}. Since our sample N={N} is capable of detecting\")\n",
    "    print(f\"effects as small as r={mde_80:.3f}, this finding is robust.\")\n",
    "elif ci_hi < 0:\n",
    "    print(\"CONCLUSION: Significant Negative Correlation.\")\n",
    "else:\n",
    "    print(\"CONCLUSION: No Significant Correlation detected.\")\n",
    "    if mde_80 < 0.1:\n",
    "        print(\"However, our analysis shows we had Power to detect even a small effect (r=0.1).\")\n",
    "        print(\"Therefore, the lack of significance suggests the true effect is likely negligible, not just missed.\")\n",
    "    else:\n",
    "        print(\"Warning: Our sample size might have been too small to detect weak effects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- 1. Helper Functions (Math remains the same) ---\n",
    "def fisher_z(r: float) -> float:\n",
    "    \"\"\"Fisher z-transform of a correlation.\"\"\"\n",
    "    # Clip r to avoid inf/nan if r is exactly 1 or -1\n",
    "    r = np.clip(r, -0.999999, 0.999999)\n",
    "    return 0.5 * np.log((1 + r) / (1 - r))\n",
    "\n",
    "def fisher_z_inv(z: float) -> float:\n",
    "    \"\"\"Inverse Fisher z-transform.\"\"\"\n",
    "    return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "\n",
    "def get_minimum_detectable_effect(N: int, power: float = 0.8, alpha: float = 0.05) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the smallest correlation (rho) that can be detected\n",
    "    with specific Power and Alpha, given Sample Size N.\n",
    "    \"\"\"\n",
    "    # Rearranging the sample size formula:\n",
    "    # sqrt(N - 3) = (z_alpha + z_beta) / z_rho\n",
    "    # z_rho = (z_alpha + z_beta) / sqrt(N - 3)\n",
    "    \n",
    "    z_alpha = norm.ppf(1 - alpha / 2) # Two-sided\n",
    "    z_beta = norm.ppf(power)\n",
    "    \n",
    "    z_rho_required = (z_alpha + z_beta) / np.sqrt(N - 3)\n",
    "    return fisher_z_inv(z_rho_required)\n",
    "\n",
    "def ci_for_correlation(r: float, N: int, alpha: float = 0.05) -> tuple[float, float]:\n",
    "    \"\"\"95% CI for a sample correlation.\"\"\"\n",
    "    z = fisher_z(r)\n",
    "    se = 1 / np.sqrt(N - 3)\n",
    "    z_alpha = norm.ppf(1 - alpha / 2)\n",
    "    z_lo = z - z_alpha * se\n",
    "    z_hi = z + z_alpha * se\n",
    "    return fisher_z_inv(z_lo), fisher_z_inv(z_hi)\n",
    "\n",
    "# --- 2. Prepare Data ---\n",
    "# Ensure we drop NaNs so N is accurate\n",
    "df_clean = curr_battle_data.dropna(subset=[\"winner\", \"conv_metadata\"])\n",
    "Y = (df_clean[\"winner\"] == \"model_a\").astype(int)\n",
    "# Extract support counts safely\n",
    "X = df_clean[\"conv_metadata\"].apply(lambda x: x.get(\"irrelevant_count_a\", 0) - x.get(\"irrelevant_count_b\", 0))\n",
    "\n",
    "# --- 3. Run Analysis ---\n",
    "N = len(X)\n",
    "r_hat = np.corrcoef(X, Y)[0, 1]\n",
    "\n",
    "print(f\"--- ANALYSIS RESULTS (N={N}) ---\")\n",
    "print(f\"Observed Correlation (r): {r_hat:.4f}\")\n",
    "\n",
    "# A. Confidence Interval (The \"Precision\" Argument)\n",
    "ci_lo, ci_hi = ci_for_correlation(r_hat, N)\n",
    "print(f\"95% Confidence Interval: [{ci_lo:.4f}, {ci_hi:.4f}]\")\n",
    "\n",
    "# B. Sensitivity Analysis (The \"Robustness\" Argument)\n",
    "# What is the smallest effect we theoretically COULD see?\n",
    "mde_80 = get_minimum_detectable_effect(N, power=0.80)\n",
    "print(f\"\\n--- SENSITIVITY ANALYSIS ---\")\n",
    "print(f\"With N={N}, you have 80% power to detect a correlation as small as r = {mde_80:.4f}\")\n",
    "\n",
    "# C. Contextual Comparison\n",
    "# Cohen's conventions: 0.1 (small), 0.3 (medium), 0.5 (large)\n",
    "print(f\"Can you detect a 'Small' effect (r=0.10)? {'YES' if mde_80 < 0.10 else 'NO'}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# D. Interpretation for Paper\n",
    "if ci_lo > 0:\n",
    "    print(\"CONCLUSION: Significant Positive Correlation.\")\n",
    "    print(f\"We detected an effect of r={r_hat:.3f}. Since our sample N={N} is capable of detecting\")\n",
    "    print(f\"effects as small as r={mde_80:.3f}, this finding is robust.\")\n",
    "elif ci_hi < 0:\n",
    "    print(\"CONCLUSION: Significant Negative Correlation.\")\n",
    "else:\n",
    "    print(\"CONCLUSION: No Significant Correlation detected.\")\n",
    "    if mde_80 < 0.1:\n",
    "        print(\"However, our analysis shows we had Power to detect even a small effect (r=0.1).\")\n",
    "        print(\"Therefore, the lack of significance suggests the true effect is likely negligible, not just missed.\")\n",
    "    else:\n",
    "        print(\"Warning: Our sample size might have been too small to detect weak effects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_diff = curr_battle_data[\"conv_metadata\"].apply(lambda x: x[\"support_count_a\"] - x[\"support_count_b\"])\n",
    "np.corrcoef(votes, support_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_diff = curr_battle_data[\"conv_metadata\"].apply(lambda x: x[\"irrelevant_count_a\"] - x[\"irrelevant_count_b\"])\n",
    "np.corrcoef(votes, irrelevant_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contradict_diff = curr_battle_data[\"conv_metadata\"].apply(lambda x: x[\"contradict_count_a\"] - x[\"contradict_count_b\"])\n",
    "np.corrcoef(votes, contradict_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.96 + 0.84) / (0.5 * np.log((1+0.017)/(1-0.017))) ** 2 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1.96 + 0.84) / (0.5 * np.log((1+0.27)/(1-0.27))) ** 2 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --- Step 1: Prepare the Simplified Data ---\n",
    "# We look at the difference: (Citations A - Citations B)\n",
    "# and the Outcome: Did A win? (1 or 0)\n",
    "df_sim = curr_battle_data.copy()\n",
    "df_sim['diff_support'] = df_sim['support_count_a'] - df_sim['support_count_b']\n",
    "df_sim['win_a'] = (df_sim['winner'] == 'model_a').astype(int)\n",
    "\n",
    "# --- Step 2: Measure the Observed Effect Size (Beta) ---\n",
    "# We fit a simple Logistic Regression to find the \"Slope\"\n",
    "X = df_sim['diff_support']\n",
    "y = df_sim['win_a']\n",
    "X_const = sm.add_constant(X) # Adds intercept\n",
    "\n",
    "model_obs = sm.Logit(y, X_const)\n",
    "result_obs = model_obs.fit(disp=0)\n",
    "\n",
    "observed_beta = result_obs.params['diff_support']\n",
    "observed_pvalue = result_obs.pvalues['diff_support']\n",
    "\n",
    "print(f\"--- OBSERVED DATA (N=800) ---\")\n",
    "print(f\"Observed Coefficient (Beta): {observed_beta:.4f}\")\n",
    "print(f\"Observed P-value: {observed_pvalue:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Step 3: Run Power Simulation at N=800 ---\n",
    "# We simulate 1000 experiments with the SAME effect size to see how robust N=800 is.\n",
    "\n",
    "n_simulations = 1000\n",
    "sample_size = 800\n",
    "detected_count = 0\n",
    "\n",
    "# Statistics for the simulation (to match your data's distribution)\n",
    "mu_x = X.mean()\n",
    "std_x = X.std()\n",
    "\n",
    "print(f\"Running {n_simulations} simulations with Beta={observed_beta:.4f}...\")\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    # 1. Generate synthetic feature (Diff in Citations) matching your data's spread\n",
    "    sim_x = np.random.normal(mu_x, std_x, sample_size)\n",
    "    \n",
    "    # 2. Generate synthetic Outcome (Win/Loss) based on the Observed Beta\n",
    "    # Logit = Intercept + Beta*X (We assume intercept 0 for balanced simulation)\n",
    "    logits = 0 + observed_beta * sim_x\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    sim_y = np.random.binomial(1, probs)\n",
    "    \n",
    "    # 3. Fit Logistic Regression to synthetic data\n",
    "    try:\n",
    "        sim_X_const = sm.add_constant(sim_x)\n",
    "        sim_model = sm.Logit(sim_y, sim_X_const)\n",
    "        sim_result = sim_model.fit(disp=0)\n",
    "        \n",
    "        # 4. Did we detect it? (p < 0.05)\n",
    "        if sim_result.pvalues[1] < 0.05:\n",
    "            detected_count += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "power = detected_count / n_simulations\n",
    "print(f\"\\n--- RESULTS ---\")\n",
    "print(f\"Statistical Power at N=800: {power*100:.1f}%\")\n",
    "\n",
    "# --- Step 4: Visual Output ---\n",
    "fig = go.Figure(go.Indicator(\n",
    "    mode = \"gauge+number\",\n",
    "    value = power * 100,\n",
    "    title = {'text': \"Statistical Power (N=800)\"},\n",
    "    gauge = {\n",
    "        'axis': {'range': [0, 100]},\n",
    "        'bar': {'color': \"black\"},\n",
    "        'steps': [\n",
    "            {'range': [0, 50], 'color': \"lightgray\"},\n",
    "            {'range': [50, 80], 'color': \"gray\"},\n",
    "            {'range': [80, 100], 'color': \"#2ca02c\"}], # Green zone\n",
    "        'threshold': {\n",
    "            'line': {'color': \"red\", 'width': 4},\n",
    "            'thickness': 0.75,\n",
    "            'value': 80}\n",
    "    }\n",
    "))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --- Step 1: Prepare the Simplified Data ---\n",
    "# We look at the difference: (Citations A - Citations B)\n",
    "# and the Outcome: Did A win? (1 or 0)\n",
    "df_sim = curr_battle_data.copy()\n",
    "df_sim['diff_support'] = df_sim['irrelevant_count_a'] - df_sim['irrelevant_count_b']\n",
    "df_sim['win_a'] = (df_sim['winner'] == 'model_a').astype(int)\n",
    "\n",
    "# --- Step 2: Measure the Observed Effect Size (Beta) ---\n",
    "# We fit a simple Logistic Regression to find the \"Slope\"\n",
    "X = df_sim['diff_support']\n",
    "y = df_sim['win_a']\n",
    "X_const = sm.add_constant(X) # Adds intercept\n",
    "\n",
    "model_obs = sm.Logit(y, X_const)\n",
    "result_obs = model_obs.fit(disp=0)\n",
    "\n",
    "observed_beta = result_obs.params['diff_support']\n",
    "observed_pvalue = result_obs.pvalues['diff_support']\n",
    "\n",
    "print(f\"--- OBSERVED DATA (N=800) ---\")\n",
    "print(f\"Observed Coefficient (Beta): {observed_beta:.4f}\")\n",
    "print(f\"Observed P-value: {observed_pvalue:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Step 3: Run Power Simulation at N=800 ---\n",
    "# We simulate 1000 experiments with the SAME effect size to see how robust N=800 is.\n",
    "\n",
    "n_simulations = 1000\n",
    "sample_size = 800\n",
    "detected_count = 0\n",
    "\n",
    "# Statistics for the simulation (to match your data's distribution)\n",
    "mu_x = X.mean()\n",
    "std_x = X.std()\n",
    "\n",
    "print(f\"Running {n_simulations} simulations with Beta={observed_beta:.4f}...\")\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    # 1. Generate synthetic feature (Diff in Citations) matching your data's spread\n",
    "    sim_x = np.random.normal(mu_x, std_x, sample_size)\n",
    "    \n",
    "    # 2. Generate synthetic Outcome (Win/Loss) based on the Observed Beta\n",
    "    # Logit = Intercept + Beta*X (We assume intercept 0 for balanced simulation)\n",
    "    logits = 0 + observed_beta * sim_x\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    sim_y = np.random.binomial(1, probs)\n",
    "    \n",
    "    # 3. Fit Logistic Regression to synthetic data\n",
    "    try:\n",
    "        sim_X_const = sm.add_constant(sim_x)\n",
    "        sim_model = sm.Logit(sim_y, sim_X_const)\n",
    "        sim_result = sim_model.fit(disp=0)\n",
    "        \n",
    "        # 4. Did we detect it? (p < 0.05)\n",
    "        if sim_result.pvalues[1] < 0.05:\n",
    "            detected_count += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "power = detected_count / n_simulations\n",
    "print(f\"\\n--- RESULTS ---\")\n",
    "print(f\"Statistical Power at N=800: {power*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --- Step 1: Prepare the Simplified Data ---\n",
    "# We look at the difference: (Citations A - Citations B)\n",
    "# and the Outcome: Did A win? (1 or 0)\n",
    "df_sim = curr_battle_data.copy()\n",
    "df_sim['diff_support'] = df_sim['contradict_count_a'] - df_sim['contradict_count_b']\n",
    "df_sim['win_a'] = (df_sim['winner'] == 'model_a').astype(int)\n",
    "\n",
    "# --- Step 2: Measure the Observed Effect Size (Beta) ---\n",
    "# We fit a simple Logistic Regression to find the \"Slope\"\n",
    "X = df_sim['diff_support']\n",
    "y = df_sim['win_a']\n",
    "X_const = sm.add_constant(X) # Adds intercept\n",
    "\n",
    "model_obs = sm.Logit(y, X_const)\n",
    "result_obs = model_obs.fit(disp=0)\n",
    "\n",
    "observed_beta = result_obs.params['diff_support']\n",
    "observed_pvalue = result_obs.pvalues['diff_support']\n",
    "\n",
    "print(f\"--- OBSERVED DATA (N=800) ---\")\n",
    "print(f\"Observed Coefficient (Beta): {observed_beta:.4f}\")\n",
    "print(f\"Observed P-value: {observed_pvalue:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Step 3: Run Power Simulation at N=800 ---\n",
    "# We simulate 1000 experiments with the SAME effect size to see how robust N=800 is.\n",
    "\n",
    "n_simulations = 1000\n",
    "sample_size = 800\n",
    "detected_count = 0\n",
    "\n",
    "# Statistics for the simulation (to match your data's distribution)\n",
    "mu_x = X.mean()\n",
    "std_x = X.std()\n",
    "\n",
    "print(f\"Running {n_simulations} simulations with Beta={observed_beta:.4f}...\")\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    # 1. Generate synthetic feature (Diff in Citations) matching your data's spread\n",
    "    sim_x = np.random.normal(mu_x, std_x, sample_size)\n",
    "    \n",
    "    # 2. Generate synthetic Outcome (Win/Loss) based on the Observed Beta\n",
    "    # Logit = Intercept + Beta*X (We assume intercept 0 for balanced simulation)\n",
    "    logits = 0 + observed_beta * sim_x\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    sim_y = np.random.binomial(1, probs)\n",
    "    \n",
    "    # 3. Fit Logistic Regression to synthetic data\n",
    "    try:\n",
    "        sim_X_const = sm.add_constant(sim_x)\n",
    "        sim_model = sm.Logit(sim_y, sim_X_const)\n",
    "        sim_result = sim_model.fit(disp=0)\n",
    "        \n",
    "        # 4. Did we detect it? (p < 0.05)\n",
    "        if sim_result.pvalues[1] < 0.05:\n",
    "            detected_count += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "power = detected_count / n_simulations\n",
    "print(f\"\\n--- RESULTS ---\")\n",
    "print(f\"Statistical Power at N=800: {power*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
