{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Setting Analysis\n",
    "\n",
    "**Section 3.3, Appendix F:** cross-arena deployment analysis, cross-benchmark analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import binomtest\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import rankdata, kendalltau, spearmanr, pearsonr\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binom_test(num_wins, num_losses, num_ties=0):\n",
    "    data = (num_wins + num_ties//2) * [1] + (num_losses + num_ties//2) * [0]\n",
    "    n_success = sum(data)\n",
    "    n_total = len(data)\n",
    "    result = binomtest(n_success, n_total, p=0.5, alternative='greater' if num_wins > num_losses else 'less')\n",
    "    return result\n",
    "\n",
    "def run_binom_test_for_intent(data, count_ties=False):\n",
    "    for intent in data['intent'].value_counts().sort_index().index:\n",
    "        data_intent = data[data['intent']==intent]\n",
    "        print(f\"Intent: {intent}\")\n",
    "        num_search_preferred = (data_intent['group_name']=='search_better').sum()\n",
    "        num_search_not_preferred = (data_intent['group_name']=='search_worse').sum()\n",
    "        if count_ties:\n",
    "            num_ties = (data_intent['group_name']=='tie').sum()\n",
    "            num_search_preferred += num_ties//2\n",
    "            num_search_not_preferred += num_ties//2\n",
    "        if num_search_preferred > num_search_not_preferred:\n",
    "            print(f\"Search better: {num_search_preferred} vs {num_search_not_preferred}\")\n",
    "            binary_data = [1] * num_search_preferred + [0] * num_search_not_preferred\n",
    "            result = binomtest(sum(binary_data), len(binary_data), p=0.5, alternative='greater')\n",
    "            print(result)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(f\"Search worse: {num_search_not_preferred} vs {num_search_preferred}\")\n",
    "            binary_data = [0] * num_search_preferred + [1] * num_search_not_preferred\n",
    "            result = binomtest(sum(binary_data), len(binary_data), p=0.5, alternative='greater')\n",
    "            print(result)\n",
    "            print(\"\\n\")\n",
    "\n",
    "def plot_intent_dist_preference(data, title, intent_order):\n",
    "    # --- STEP 1: Aggregate proportions ---\n",
    "    count_df = data.groupby(['group_name', 'intent']).size().reset_index(name='count')\n",
    "    total_per_intent = count_df.groupby('intent')['count'].transform('sum')\n",
    "    count_df['proportion'] = count_df['count'] / total_per_intent\n",
    "    count_df = count_df[count_df['intent'] != 'Other']\n",
    "\n",
    "    # --- STEP 2: Format group names ---\n",
    "    count_df['group_name'] = count_df['group_name'].map({\n",
    "        'tie': 'Tie',\n",
    "        'search_better': 'Search Preferred', \n",
    "        'search_worse': 'Non-search Preferred'\n",
    "    })\n",
    "\n",
    "    # --- STEP 3: Plot ---\n",
    "    fig = px.bar(\n",
    "        count_df,\n",
    "        x='intent',\n",
    "        y='count',\n",
    "        color='group_name',\n",
    "        category_orders={'intent': intent_order},\n",
    "        color_discrete_map={\n",
    "            'Tie': 'gray',\n",
    "            'Search Preferred': '#1f77b4',\n",
    "            'Non-search Preferred': '#d62728'\n",
    "        },\n",
    "        hover_data={'proportion': ':.2%', 'count': True, 'intent': False},\n",
    "        labels={'count': 'Count', 'group_name': '', 'intent': 'Intent'},\n",
    "        barmode='group',\n",
    "        opacity=0.8,\n",
    "    )\n",
    "\n",
    "    # --- STEP 4: Text and style ---\n",
    "    fig.update_traces(\n",
    "        texttemplate='%{customdata[0]:.0%}',\n",
    "        textposition='outside',\n",
    "        textfont=dict(size=26),\n",
    "        marker_line_color='black',\n",
    "        marker_line_width=1.2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        margin=dict(l=10, r=10, t=40, b=10),\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            font=dict(size=22, color='black'),\n",
    "            x=0.06\n",
    "        ),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "            font=dict(size=16)\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title='',\n",
    "            tickangle=30,\n",
    "            tickfont=dict(size=16, color='black'),\n",
    "            linecolor='black',\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=dict(text='Count', font=dict(size=18, color='black')),\n",
    "            tickfont=dict(size=14, color='black'),\n",
    "            linecolor='black',\n",
    "            gridcolor='lightgray',\n",
    "            zeroline=False\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=500\n",
    "    )\n",
    "    return fig\n",
    "    \n",
    "def remove_inline_citations(text):\n",
    "    return re.sub(r'\\s*\\[[\\d\\s,]+\\]\\s*', '', text)\n",
    "    \n",
    "def preprocess_for_describing_diff(data, intent, output_file):\n",
    "    data_intent = data[data['intent']==intent]\n",
    "    chat_col_name = 'conversation_' if 'conversation_a' in data_intent.columns else 'messages_'\n",
    "    data_intent = data_intent[data_intent['group_name']!='tie']\n",
    "    data_intent['question'] = data_intent[chat_col_name+'a'].apply(lambda x: x[0]['content'])\n",
    "    data_intent.drop_duplicates(subset=['question'], inplace=True)\n",
    "    processed = []\n",
    "    for _, row in data_intent.iterrows():\n",
    "        if any(keyword in row['model_a'] for keyword in ['grounding', 'search', 'sonar']):\n",
    "            processed.append({'question': row['question'], 'answer': remove_inline_citations(row[chat_col_name+'a'][1]['content']), 'group_name': 'search'})\n",
    "            processed.append({'question': row['question'], 'answer': remove_inline_citations(row[chat_col_name+'b'][1]['content']), 'group_name': 'non-search'})\n",
    "        else:\n",
    "            processed.append({'question': row['question'], 'answer': remove_inline_citations(row[chat_col_name+'a'][1]['content']), 'group_name': 'non-search'})\n",
    "            processed.append({'question': row['question'], 'answer': remove_inline_citations(row[chat_col_name+'b'][1]['content']), 'group_name': 'search'})\n",
    "    processed = pd.DataFrame(processed)\n",
    "    print(f\"Number of rows: {len(processed)}\")\n",
    "    processed.to_csv(output_file)\n",
    "    display(processed.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Arena Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Arena Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_arena_battles = pd.read_json('../data/text_arena_battles.jsonl', orient='records', lines=True)\n",
    "print(f'Number of rows: {len(text_arena_battles)}')\n",
    "text_arena_battles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_arena_battles['group_name'] = 'tie'\n",
    "text_arena_battles.loc[\n",
    "    ((text_arena_battles['winner']=='model_a') & (text_arena_battles['model_a']=='gemini-2.5-pro-exp-03-25-grounding-none')) |\n",
    "    ((text_arena_battles['winner']=='model_b') & (text_arena_battles['model_b']=='gemini-2.5-pro-exp-03-25-grounding-none')),\n",
    "    'group_name'] = 'search_better'\n",
    "text_arena_battles.loc[\n",
    "    ((text_arena_battles['winner']=='model_b') & (text_arena_battles['model_a']=='gemini-2.5-pro-exp-03-25-grounding-none')) |\n",
    "    ((text_arena_battles['winner']=='model_a') & (text_arena_battles['model_b']=='gemini-2.5-pro-exp-03-25-grounding-none')),\n",
    "    'group_name'] = 'search_worse'\n",
    "\n",
    "text_arena_battles['group_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_wins = (text_arena_battles['group_name']=='search_better').sum()\n",
    "num_losses = (text_arena_battles['group_name']=='search_worse').sum()\n",
    "num_ties = (text_arena_battles['group_name']=='tie').sum()\n",
    "print(f\"search preferred: {num_wins}\")\n",
    "print(f\"search not preferred: {num_losses}\")\n",
    "print(f\"ties: {num_ties}\")\n",
    "print(run_binom_test(num_wins, num_losses))\n",
    "print(\"Running binom tests broken down by intent\")\n",
    "run_binom_test_for_intent(text_arena_battles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_ORDER = ['Factual Lookup', 'Info Synthesis', 'Analysis', 'Recommendation', 'Explanation', 'Creative Generation', 'Guidance', 'Text Processing']\n",
    "fig = plot_intent_dist_preference(text_arena_battles, title='Text Arena Settings', intent_order=INTENT_ORDER)\n",
    "fig.show()\n",
    "fig.write_image(\"plots/lang_vote_dist.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_for_describing_diff(text_arena_battles, intent='Text Processing', output_file='../data/describing_diffs/lang_text_processing_qa.csv')\n",
    "preprocess_for_describing_diff(text_arena_battles, intent='Factual Lookup', output_file='../data/describing_diffs/lang_factual_qa.csv')\n",
    "preprocess_for_describing_diff(text_arena_battles, intent='Info Synthesis', output_file='../data/describing_diffs/lang_synthesis_qa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Arena Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_battles = pd.read_json('../data/search_arena_24k.jsonl', orient='records', lines=True)\n",
    "\n",
    "search_battles = search_battles[search_battles['winner'].notna()]\n",
    "search_battles = search_battles[\n",
    "    ((search_battles['model_a']=='gemini-2.5-pro-exp-03-25-wo-search') & (~search_battles['model_b'].str.contains('gpt'))) |\n",
    "    ((search_battles['model_b']=='gemini-2.5-pro-exp-03-25-wo-search') & (~search_battles['model_a'].str.contains('gpt')))\n",
    "]\n",
    "\n",
    "search_battles.rename(columns={'primary_intent': 'intent'}, inplace=True)\n",
    "print(f'Number of rows: {len(search_battles)}')\n",
    "search_battles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_battles['group_name'] = 'tie'\n",
    "search_battles.loc[\n",
    "    ((search_battles['winner']=='model_a') & (search_battles['model_a']=='gemini-2.5-pro-exp-03-25-wo-search')) |\n",
    "    ((search_battles['winner']=='model_b') & (search_battles['model_b']=='gemini-2.5-pro-exp-03-25-wo-search')),\n",
    "    'group_name'] = 'search_worse'\n",
    "search_battles.loc[\n",
    "    ((search_battles['winner']=='model_b') & (search_battles['model_a']=='gemini-2.5-pro-exp-03-25-wo-search')) |\n",
    "    ((search_battles['winner']=='model_a') & (search_battles['model_b']=='gemini-2.5-pro-exp-03-25-wo-search')),\n",
    "    'group_name'] = 'search_better'\n",
    "\n",
    "search_battles['group_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_wins = (search_battles['group_name']=='search_better').sum()\n",
    "num_losses = (search_battles['group_name']=='search_worse').sum()\n",
    "print(f\"search preferred: {num_wins}\")\n",
    "print(f\"search not preferred: {num_losses}\")\n",
    "print(run_binom_test(num_wins, num_losses))\n",
    "print(\"Running binom tests broken down by intent\")\n",
    "run_binom_test_for_intent(search_battles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_ORDER = ['Factual Lookup', 'Info Synthesis', 'Analysis', 'Recommendation', 'Explanation', 'Creative Generation', 'Guidance', 'Text Processing']\n",
    "fig = plot_intent_dist_preference(search_battles, title='Search Arena Settings', intent_order=INTENT_ORDER)\n",
    "fig.show()\n",
    "fig.write_image(\"plots/search_vote_dist.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_for_describing_diff(search_battles, intent='Factual Lookup', output_file='../data/describing_diffs/search_factual_qa.csv')\n",
    "preprocess_for_describing_diff(search_battles, intent='Info Synthesis', output_file='../data/describing_diffs/search_synthesis_qa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt differences across datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_arena = pd.read_json('../data/search_arena_24k.jsonl', orient='records', lines=True)\n",
    "print(f\"Number of rows: {len(search_arena)}\")\n",
    "search_arena.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_arena = load_dataset(\"lmarena-ai/arena-human-preference-100k\", split=\"train\")\n",
    "text_arena = text_arena.to_pandas()\n",
    "print(f\"Number of rows: {len(text_arena)}\")\n",
    "text_arena.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleqa = pd.read_csv(\"https://openaipublic.blob.core.windows.net/simple-evals/simple_qa_test_set.csv\")\n",
    "print(f\"Number of rows: {len(simpleqa)}\")\n",
    "simpleqa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "\n",
    "def derive_key(password: str, length: int) -> bytes:\n",
    "    \"\"\"Derive a fixed-length key from the password using SHA256.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    hasher.update(password.encode())\n",
    "    key = hasher.digest()\n",
    "    return key * (length // len(key)) + key[: length % len(key)]\n",
    "\n",
    "def decrypt(ciphertext_b64: str, password: str) -> str:\n",
    "    \"\"\"Decrypt base64-encoded ciphertext with XOR.\"\"\"\n",
    "    encrypted = base64.b64decode(ciphertext_b64)\n",
    "    key = derive_key(password, len(encrypted))\n",
    "    decrypted = bytes(a ^ b for a, b in zip(encrypted, key))\n",
    "    return decrypted.decode()\n",
    "\n",
    "browsecomp = pd.read_csv(\"https://openaipublic.blob.core.windows.net/simple-evals/browse_comp_test_set.csv\")\n",
    "for index in browsecomp.index:\n",
    "    row = browsecomp.iloc[index]\n",
    "    browsecomp.loc[index, 'problem'] = decrypt(row.get(\"problem\", \"\"), row.get(\"canary\", \"\"))\n",
    "    browsecomp.loc[index, 'answer'] = decrypt(row.get(\"answer\", \"\"), row.get(\"canary\", \"\"))\n",
    "print(f\"Number of rows: {len(browsecomp)}\")\n",
    "browsecomp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating comparison datasets for describing diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search vs lm arena\n",
    "search_arena_english = search_arena[search_arena['languages'].apply(lambda x: x[0]=='English' if len(x)>0 else False)]\n",
    "text_arena_english = text_arena[text_arena['language']=='English'].sample(n=len(search_arena_english))\n",
    "search_arena_prompts = list(search_arena_english['messages_a'].apply(lambda x: x[0]['content']))\n",
    "text_arena_prompts = list(text_arena_english['conversation_a'].apply(lambda x: x[0]['content']))\n",
    "search_arena_vs_text_arena = []\n",
    "for prompt in search_arena_prompts:\n",
    "    search_arena_vs_text_arena.append(\n",
    "        {\n",
    "            \"text\": prompt,\n",
    "            \"group_name\": \"search_arena\",\n",
    "        }\n",
    "    )\n",
    "for prompt in text_arena_prompts:\n",
    "    search_arena_vs_text_arena.append(\n",
    "        {\n",
    "            \"text\": prompt,\n",
    "            \"group_name\": \"language_arena\",\n",
    "        }\n",
    "    )\n",
    "search_arena_vs_language_arena = pd.DataFrame(search_arena_vs_text_arena)\n",
    "search_arena_vs_language_arena.to_csv('../data/describing_diffs/search_arena_vs_text_arena.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search vs simpleqa\n",
    "simpleqa_prompts = list(simpleqa['problem'])\n",
    "search_arena_english = search_arena[search_arena['languages'].apply(lambda x: x[0]=='English' if len(x)>0 else False)].sample(len(simpleqa_prompts))\n",
    "search_arena_prompts = list(search_arena_english['messages_a'].apply(lambda x: x[0]['content']))\n",
    "search_arena_vs_simpleqa = []\n",
    "for prompt in search_arena_prompts:\n",
    "    search_arena_vs_simpleqa.append(\n",
    "        {\n",
    "            \"text\": prompt,\n",
    "            \"group_name\": \"search_arena\",\n",
    "        }\n",
    "    )\n",
    "for prompt in simpleqa_prompts:\n",
    "    search_arena_vs_simpleqa.append(\n",
    "        {\n",
    "            \"text\": prompt,\n",
    "            \"group_name\": \"simpleqa\",\n",
    "        }\n",
    "    )\n",
    "search_arena_vs_simpleqa = pd.DataFrame(search_arena_vs_simpleqa)\n",
    "search_arena_vs_simpleqa.to_csv('../data/describing_diffs/search_arena_vs_simpleqa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search vs browsecomp\n",
    "browsecomp_prompts = list(browsecomp['problem'])\n",
    "search_arena_english = search_arena[search_arena['languages'].apply(lambda x: x[0]=='English' if len(x)>0 else False)].sample(n=len(browsecomp))\n",
    "search_arena_prompts = list(search_arena_english['messages_a'].apply(lambda x: x[0]['content']))\n",
    "search_arena_vs_browsecomp = []\n",
    "for prompt in search_arena_prompts:\n",
    "    search_arena_vs_browsecomp.append(\n",
    "        {\n",
    "            \"text\": prompt,\n",
    "            \"group_name\": \"search_arena\",\n",
    "        }\n",
    "    )\n",
    "for prompt in browsecomp_prompts:\n",
    "    search_arena_vs_browsecomp.append(\n",
    "        {\n",
    "            \"text\": prompt,\n",
    "            \"group_name\": \"browsecomp\",\n",
    "        }\n",
    "    )\n",
    "search_arena_vs_browsecomp = pd.DataFrame(search_arena_vs_browsecomp)\n",
    "search_arena_vs_browsecomp.to_csv('../data/describing_diffs/search_arena_vs_browsecomp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Benchmark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"sonar-reasoning-pro-high\",\n",
    "    \"gemini-2.5-pro-grounding\",\n",
    "    \"sonar-pro-high\",\n",
    "    \"sonar-reasoning\",\n",
    "    \"sonar-pro\",\n",
    "    \"sonar\",\n",
    "    \"gemini-2.5-flash-grounding\",\n",
    "    \"gemini-2.0-flash-grounding\",\n",
    "    \"gpt-4o-search-preview-high\",\n",
    "    \"gpt-4o-mini-search-preview\"    \n",
    "]\n",
    "\n",
    "searcharena_scores = [66.6, 66.7, 63.7, 60.1, 57.5, 55.8, 49.8, 41.7, 34.8, 29.3]\n",
    "searcharena_factual_scores = [68.8, 68.3, 59.9, 58.8, 56.8, 56.8, 54.8, 54.8, 42.8, 36.2, 30.3]\n",
    "searcharena_nonfactual_scores = [65.0, 65.3, 66.7, 61.3, 58.4, 56.8, 46.1, 41.0, 33.6, 28.6]\n",
    "simpleqa_scores = [91.8, 90.6, 93.2, 92.6, 92.6, 91.8, 89.8, 89.2, 89.4, 91.2]\n",
    "arenahard_scores = [28.3, 33.5, 43.4, 29.1, 39.9, 36.2, 22.6, 13.4, 16.6, 8.3]\n",
    "\n",
    "include_idx = [0, 1, 2, 5, 6, 8, 9]\n",
    "categories = [\"Search Arena\", \"Search Arena (Fact+Synth)\", \"SimpleQA\", \"ArenaHard\", \"Search Arena (Other)\"]\n",
    "\n",
    "colors = px.colors.qualitative.Vivid\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, idx in enumerate(include_idx):\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=[\n",
    "            searcharena_scores[idx],\n",
    "            searcharena_factual_scores[idx],\n",
    "            simpleqa_scores[idx],\n",
    "            arenahard_scores[idx],\n",
    "            searcharena_nonfactual_scores[idx],\n",
    "            searcharena_scores[idx],\n",
    "        ],\n",
    "        theta=categories + [categories[0]],\n",
    "        name=models[idx],\n",
    "        line=dict(color=colors[i % len(colors)], width=2)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            tickfont=dict(size=14, color='black'),\n",
    "            tickvals=[0.0, 20, 40, 60, 80, 100],\n",
    "        range=[0, 100],\n",
    "        ),\n",
    "        angularaxis=dict(\n",
    "            tickfont=dict(size=18, color='black'),\n",
    "            rotation=90,\n",
    "            direction=\"clockwise\"\n",
    "        )\n",
    "    ),\n",
    "    legend=dict(\n",
    "        font=dict(size=16, color='black'),\n",
    "        orientation=\"v\",\n",
    "        yanchor=\"top\",\n",
    "        y=1.1,\n",
    "        xanchor=\"left\",\n",
    "        x=0.8,\n",
    "    ),\n",
    "    template=\"plotly_white\",\n",
    "    margin=dict(r=20, t=20, b=20, l=20),\n",
    "    width=1000,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"plots/cross_benchmark_scores.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcharena_scores = [66.6, 66.7, 63.7, 60.1, 57.5, 55.8, 49.8, 41.7, 34.8, 29.3]\n",
    "searcharena_factual_scores = [68.8, 68.3, 59.9, 58.8, 56.8, 56.8, 54.8, 54.8, 42.8, 36.2]\n",
    "searcharena_nonfactual_scores = [65.0, 65.3, 66.7, 61.3, 58.4, 56.8, 46.1, 41.0, 33.6, 28.6]\n",
    "simpleqa_scores = [91.8, 90.6, 93.2, 92.6, 92.6, 91.8, 89.8, 89.2, 89.4, 91.2]\n",
    "arenahard_scores = [28.3, 33.5, 43.4, 29.1, 39.9, 36.2, 22.6, 13.4, 16.6, 8.3]\n",
    "\n",
    "score_lists = {\n",
    "    \"SearchArena\": searcharena_scores,\n",
    "    \"SearchArena (Factual)\": searcharena_factual_scores,\n",
    "    \"SearchArena (Nonfactual)\": searcharena_nonfactual_scores,\n",
    "    \"SimpleQA\": simpleqa_scores,\n",
    "    \"ArenaHard\": arenahard_scores\n",
    "}\n",
    "\n",
    "ranked_lists = {name: rankdata([-s for s in scores], method='ordinal') for name, scores in score_lists.items()}\n",
    "results = []\n",
    "\n",
    "for (name1, r1), (name2, r2) in combinations(ranked_lists.items(), 2):\n",
    "    raw1 = score_lists[name1]\n",
    "    raw2 = score_lists[name2]\n",
    "\n",
    "    tau, _ = kendalltau(r1, r2)\n",
    "    rho, _ = spearmanr(r1, r2)\n",
    "    pearson, _ = pearsonr(raw1, raw2)\n",
    "\n",
    "    results.append(((name1, name2), tau, rho, pearson))\n",
    "\n",
    "for (name1, name2), tau, rho, pearson in results:\n",
    "    print(f\"{name1} vs {name2}:\\n\"\n",
    "          f\"  Kendall Tau   = {tau:.3f}\\n\"\n",
    "          f\"  Spearman Rho  = {rho:.3f}\\n\"\n",
    "          f\"  Pearson Correlation = {pearson:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arenahard_scores = {\n",
    "    \"gemini-2.5-pro\": {\"search\": (33.5, -2.1, 2.4), \"non-search\": (51.8, -2.6, 2.3)},\n",
    "    \"gemini-2.5-flash\": {\"search\": (22.6, -1.4, 1.8), \"non-search\": (45.5, -2.3, 2.3)},\n",
    "    \"gemini-2.0-flash\": {\"search\": (13.4, -1.0, 1.1), \"non-search\": (19.5, -1.4, 1.2)}, \n",
    "}\n",
    "\n",
    "simpleqa_scores = {\n",
    "    \"gemini-2.5-pro\": {\"search\": (90.6, -2.6, 2.4), \"non-search\": (48.6, -4.4, 4.4)},\n",
    "    \"gemini-2.5-flash\": {\"search\": (89.8, -2.6, 2.6), \"non-search\": (31.4, -4.0, 4.2)},\n",
    "    \"gemini-2.0-flash\": {\"search\": (89.2, -2.8, 2.6), \"non-search\": (26.4, -3.8, 3.8)},\n",
    "}\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "marker_map = {\"search\": \"circle\", \"non-search\": \"square\"}\n",
    "neutral_color = \"#888888\"\n",
    "fig = go.Figure()\n",
    "x_vals, y_vals = [], []\n",
    "\n",
    "for i, model in enumerate(arenahard_scores):\n",
    "    color = colors[i % len(colors)]\n",
    "    for version in [\"search\", \"non-search\"]:\n",
    "        x, x_low, x_high = simpleqa_scores[model][version]\n",
    "        y, y_low, y_high = arenahard_scores[model][version]\n",
    "\n",
    "        x_vals.append(x)\n",
    "        y_vals.append(y)\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[x],\n",
    "            y=[y],\n",
    "            mode='markers+text',\n",
    "            showlegend=False,\n",
    "            marker=dict(\n",
    "                symbol=marker_map[version],\n",
    "                size=10,\n",
    "                color=color,\n",
    "                line=dict(width=1, color='black')\n",
    "            ),\n",
    "            error_x=dict(\n",
    "                type='data',\n",
    "                symmetric=False,\n",
    "                array=[x_high],\n",
    "                arrayminus=[abs(x_low)],\n",
    "                thickness=1.5\n",
    "            ),\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                symmetric=False,\n",
    "                array=[y_high],\n",
    "                arrayminus=[abs(y_low)],\n",
    "                thickness=1.5\n",
    "            ),\n",
    "            text=[model],\n",
    "            textposition=\"top left\" if version == \"search\" else \"top right\",\n",
    "            textfont=dict(size=16, color='black')\n",
    "        ))\n",
    "\n",
    "for version in marker_map:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[None], y=[None],\n",
    "        mode='markers',\n",
    "        name=version,\n",
    "        marker=dict(\n",
    "            symbol=marker_map[version],\n",
    "            size=12,\n",
    "            color=neutral_color,\n",
    "            line=dict(width=1, color='black')\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        hoverinfo='skip',\n",
    "    ))\n",
    "\n",
    "X = np.array(x_vals).reshape(-1, 1)\n",
    "y = np.array(y_vals)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "slope = reg.coef_[0]\n",
    "intercept = reg.intercept_\n",
    "line_x = np.linspace(min(x_vals), max(x_vals), 100)\n",
    "line_y = slope * line_x + intercept\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=line_x,\n",
    "    y=line_y,\n",
    "    mode='lines',\n",
    "    name=f\"Fit: y = {slope:.2f}x + {intercept:.2f}\",\n",
    "    line=dict(color='black', dash='dash'),\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title=dict(text=\"SimpleQA Score\", font=dict(size=16, color='black')),\n",
    "        tickfont=dict(size=16, color='black'),\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text=\"ArenaHard Score\", font=dict(size=16, color='black')),\n",
    "        tickfont=dict(size=16, color='black'),\n",
    "    ),\n",
    "    legend=dict(\n",
    "        font=dict(size=16, color='black'),\n",
    "        orientation=\"v\",\n",
    "        yanchor=\"top\",\n",
    "        y=0.9,\n",
    "        xanchor=\"left\",\n",
    "    ),\n",
    "    template=\"plotly_white\",\n",
    "    margin=dict(r=20, t=20, b=20, l=20),\n",
    "    width=1000,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"plots/simpleqa_vs_arenahard.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
